---
layout:     post
title:      大模型笔记（LLM）
subtitle:   大语言模型基础2
date:       2024-9-4
author:     月月鸟
header-img: img/LLM.jpg
catalog: true
tags:
    - LLM
---

# 5. 词性标注

## 5.1. 概述

词性标注是自然语言处理中的基础任务之一，它为句法分析、信息提取等高级应用奠定了基础。与分词类似，中文的词性标注也面临不少挑战，比如同一个词语可能具有多种词性，以及如何处理未登录词。借助字典查询和基于统计的方法，能够有效应对这些问题。通常，词性标注是在分词之后进行的。

## 5.2. 词性标注的难点

词性标注是识别词语在句子中的语法角色，它涉及多种类型的词性。ICTCLAS是目前较为广泛采用的中文词性标注标准，其词性类别定义可以参考相关资料。词性标注的主要难点包括：

- **缺乏词形变化**：与英语不同，汉语中的词通常没有形态变化，无法通过词形判断词性。
- **多词性现象**：同一个词可以在不同语境中拥有不同的词性。例如，“研究”既可以是名词（如“基础研究”），也可以是动词（如“研究新的方法”）。研究显示，22.5%的中文词语具有多种词性，且常用词的多词性现象尤为突出。
- **标准不统一**：不同的语料库使用的词性划分标准和标注符号有所差异，给词性标注带来了不小的困难。比如LDC语料库中将一级词性划分为33类，而北京大学语料库则划分为26类。jieba分词采用了较为常见的ICTCLAS标准。
- **未登录词问题**：类似于分词中的未登录词，词性标注也需要处理这些未出现在词典中的新词。对这些词语无法通过查词典来获取词性，可以利用HMM隐马尔科夫模型等统计方法来识别。

## 5.3. 词性标注算法

词性标注的方法与分词类似，可以分为两大类：基于字典的查找方法和基于统计的模型方法。jieba分词结合了这两种方法，对于已知的词语通过字典查询其词性，对于未登录词则采用HMM模型进行处理。

### 5.3.1 基于字典的词性查找

在这种方法中，首先对文本进行分词，然后通过查找词典来确定每个词的词性。jieba在对分词结果进行标注时就使用了这种方式。虽然这种方法简单直观，但它无法很好地处理多词性现象，会导致一定程度的误差。

### 5.3.2 基于统计的词性标注

和分词类似，HMM隐马尔科夫模型也可以用于词性标注。这里，分词后的文本是观测序列，而标注后的词性是隐藏序列。HMM模型中起始概率、发射概率和转移概率与分词模型中的意义相似，都可以通过大规模语料训练得到。使用Viterbi算法计算最可能的隐藏序列，即可完成词性标注。

## 5.4. jieba词性标注原理

jieba不仅能够进行分词，还可以同时完成词性标注。通过`jieba.posseg`模块，可以对每个分词后的词语进行词性标注，并且采用的是与ICTCLAS兼容的标注集。以下是一个使用示例：

```python
import jieba.posseg as pseg

words = pseg.cut("我爱北京天安门")
for word, flag in words:
    print(f'{word} {flag}')
```

输出结果：

- 我 r  （代词）
- 爱 v  （动词）
- 北京 ns  （名词）
- 天安门 ns  （名词）

### 5.4.1 准备工作

在标注之前，jieba会检查字典是否已经加载，如果尚未加载则进行初始化。接着将输入文本转为UTF-8或GBK编码，然后通过正则表达式将文本切分为短语段落。根据用户的设置，选择是否使用HMM模型进行后续处理。

### 5.4.2 分词与词性标注

jieba在处理每个短语时，会构建一个DAG（有向无环图），利用动态规划计算每个节点到句末的最佳分词路径。然后根据这些路径生成最终的分词结果，并通过查询词典为每个词语标注词性。对于未登录词，则使用HMM模型进一步处理。

```python
def __cut_DAG(self, sentence):
    DAG = self.tokenizer.get_DAG(sentence)
    route = {}
    self.tokenizer.calc(sentence, DAG, route)
    x = 0
    buf = ''
    N = len(sentence)
    while x < N:
        y = route[x][1] + 1
        l_word = sentence[x:y]
        if y - x == 1:
            buf += l_word
        else:
            if buf:
                yield pair(buf, self.word_tag_tab.get(buf, 'x'))
                buf = ''
            yield pair(l_word, self.word_tag_tab.get(l_word, 'x'))
        x = y
    if buf:
        yield pair(buf, self.word_tag_tab.get(buf, 'x'))
```

### 5.4.3 未登录词处理

对于词典中未包含的新词，jieba利用HMM隐马尔科夫模型进行处理。模型通过大规模语料库训练，生成起始概率、发射概率和转移概率。Viterbi算法在这三者的基础上，找到最可能的词性标注路径，从而完成未登录词的标注。

```python
def __cut(self, sentence):
    prob, pos_list = viterbi(sentence, char_state_tab_P, start_P, trans_P, emit_P)
    for i, char in enumerate(sentence):
        pos = pos_list[i][0]
        if pos == 'B':
            begin = i
        elif pos == 'E':
            yield pair(sentence[begin:i + 1], pos_list[i][1])
        elif pos == 'S':
            yield pair(char, pos_list[i][1])
```

Viterbi算法的作用是通过概率计算，为每个词语找到最合适的词性标签。

jieba分词在完成分词的同时，也能进行词性标注，因此在速度和效率上表现出色。对于词典中已有的词语，jieba通过查找词典进行标注；对于未登录词，则依赖HMM模型来推断词性。不过，字典查找的方法无法处理一词多词性的情况，因此在某些复杂语境下，标注精度还存在提升空间。通过结合字典查找和统计模型，jieba在中文分词与词性标注上达到了较高的实用性与准确度，是处理中文文本时不可多得的利器。

---

# 6. 句法分析

## 6.1. 概述

句法分析是自然语言处理（NLP）中的关键步骤，它旨在解析句子的结构，识别出句中的主谓宾关系，以及词语之间的依存关系（如并列、从属、递进等）。句法分析为语义分析、情感分析和观点提取等应用提供了重要支持。

虽然随着深度学习技术的发展，尤其是具有内部语法感知能力的LSTM模型的应用，句法分析在一些任务中变得不再是必需品，但在处理结构复杂的长句子或标注数据较少的场景中，句法分析依然有其独特价值。因此，研究句法分析仍然十分重要。

## 6.2. 句法分析的分类

句法分析主要分为两大类：句法结构分析和语义依存关系分析。每种分析方式侧重的内容不同，但都在理解语言的过程中扮演着重要角色。

![](https://dongnian.icu/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/4.%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/image/image_0FMBp4fbBs.png)

### 6.2.1 句法结构分析

句法结构分析主要关注句子中的主谓宾、定状补等成分的识别，以及它们之间的关系。通过这种分析方式，可以提取出句子的核心结构，帮助我们更好地理解句子的主要含义和层次关系。

在处理复杂句子时，仅依靠词性分析往往无法准确地识别各个成分之间的关系，而句法结构分析则可以清楚地展现出这些关系。比如，它可以识别出句子中谁是动作的执行者（主语），动作本身是什么（谓语），以及动作的对象（宾语）。

### 6.2.2 语义依存关系分析

语义依存关系分析则侧重于揭示词汇之间的从属、并列、递进等关系，提供更深层次的语义信息。例如，不同的句子可能用不同的表达方式来传达相同的含义，而语义依存关系分析可以识别出这些表达之间的内在相似性。

这类分析注重介词、连词等非实词在句子中的作用。与此相比，句法结构分析则更关注名词、动词、形容词等实词之间的关系。例如，在“张三吃苹果”这句话中，“张三”与“吃”之间的关系是施事关系（Agt），而“苹果”与“吃”之间则是受事关系（Pat）。通过语义依存关系分析，我们可以更准确地理解句子表达的实际含义。

## 6.3. 句法分析工具

句法分析本身比较复杂，但有一些工具可以帮助我们进行句法分析。以下是几个常见的句法分析工具：

- **哈工大LTP（语言技术平台云 LTP-Cloud）**：哈工大开发的LTP是一款中文处理工具，提供包括分词、词性标注、句法分析在内的多种功能。
- **斯坦福句法分析工具（Stanford Parser）**：斯坦福大学的NLP小组开发的Stanford Parser，支持多种语言的句法分析，广泛应用于学术研究和工业界。

尽管这些工具可以帮助我们进行句法分析，但在复杂句子上的准确度仍然有限，例如哈工大的LTP的准确度约为80%左右。句法分析的难度依然很高，提升准确率是一个持续的挑战。

## 6.4. 深度学习与句法分析

深度学习，尤其是RNN和LSTM等序列模型，在NLP领域的广泛应用，使得句法分析在某些场景中的必要性有所减弱。这是因为这些模型在处理文本时，能够自动学习和捕捉句法结构和依存关系的深层信息。

此外，句法分析树也可以与深度学习结合，形成树结构的LSTM模型（Tree-LSTM），用于文本摘要、情感分析等任务。然而，Tree-LSTM是否比传统的双向LSTM（Bi-LSTM）更有效，并不总是如此。

研究表明，许多情况下，Bi-LSTM的效果反而优于Tree-LSTM。这主要是由于当前句法分析的准确度仍然不高，约为90%左右。在句子结构复杂的情况下，误差可能更大，导致噪声的引入，从而影响Tree-LSTM的性能。但Tree-LSTM在处理长句子和样本较少的场景时，表现往往更出色。可以参考哈工大车万翔的研究《自然语言处理中的深度学习模型是否依赖于树结构》进一步了解这方面的讨论。

![](https://dongnian.icu/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/4.%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/image/image_qmnRnNmiSj.png)

## 6.5. 总结

句法分析是自然语言处理中的重要基础工作，在文本分析、情感分析、观点提取等领域具有广泛的应用价值。尽管当前句法分析仍然面临准确率的提升难题，但它在处理复杂语句结构时，依然能够提供深刻的洞察。随着深度学习的进步，结合句法结构树的模型有望进一步提高NLP任务的表现。总的来说，句法分析是一项长期而富有挑战性的研究方向，仍有广阔的探索空间。

---

# 7. 词向量

## 7.1. 概述

词向量是自然语言处理（NLP）领域中的核心技术之一，它将词汇映射到一个连续的、低维的实数空间中，使得计算机能够理解和处理语言的语义信息。通过这种方式，词向量可以捕捉词汇之间的语义相似性和关系，从而使得机器能够更智能地处理文本数据。

传统的文本处理方法中，词语通常是离散的符号，计算机很难直接理解其内在含义。而词向量的出现，使得每个词都可以表示为一个特征向量，这些向量在语义空间中具备数学性质，可以进行相似性计算、聚类、分类等操作。

## 7.2. 词向量的概念

词向量的基本思想是，将每个词映射到一个固定大小的向量空间中，通常是几百维或几千维的实数向量。在这个向量空间中，词语之间的语义关系可以通过向量之间的距离或角度来衡量。例如，“国王”和“王后”这两个词的向量距离很接近，而“苹果”和“汽车”的向量距离则较远，表明它们的语义差异更大。

在实际操作中，我们希望词向量能够具备以下特性：

- **语义相似性**：语义相近的词在向量空间中距离较近。例如，“好”和“棒”应该在向量空间中有相近的表示。
- **向量操作**：可以通过向量的简单操作，得到语义上合理的结果。例如，“国王 - 男人 + 女人 ≈ 王后”，通过这种简单的加减法，模型可以自动学习到性别、类别等隐含特征。

## 7.3. 词向量生成方法

词向量的生成方法主要可以分为两类：基于统计的方法和基于神经网络的方法。

### 7.3.1 基于统计的方法

基于统计的方法主要依赖于词语在大规模语料库中的共现关系，通过统计词与词之间的共现次数，生成词的向量表示。这类方法的核心思想是：**上下文相似的词，通常具有相似的含义**。

- **共现矩阵**：最简单的统计方法是构建一个词语的共现矩阵。例如，可以统计每个词与其他词在固定窗口大小内共现的次数，得到一个词与词的共现矩阵。这种方法生成的向量维度通常较高（与词汇表大小一致），计算效率较低。

- **PCA/SVD降维**：为了减少共现矩阵的高维度，可以对共现矩阵进行主成分分析（PCA）或奇异值分解（SVD），将高维空间压缩到低维空间，从而得到每个词的低维向量表示。这种方法在早期NLP应用中有一定使用，但在语义捕捉能力上较为有限。

### 7.3.2 基于神经网络的方法

基于神经网络的方法在处理大规模语料和捕捉语义关系方面表现更好。以下是几种经典的神经网络词向量模型：

- **Word2Vec**：Word2Vec是由Mikolov等人在2013年提出的，它通过神经网络训练生成词向量。Word2Vec有两种主要训练方式：
  - **CBOW（Continuous Bag of Words）**：利用上下文词语来预测当前词语的概率。比如，给定“我/喜欢/（空）/学习”，模型通过“我”和“喜欢”来预测“学习”这个词。
  - **Skip-gram**：与CBOW相反，Skip-gram用当前词语来预测上下文词语的概率。比如，给定“学习”，模型预测其上下文词语“我”和“喜欢”。Skip-gram在处理低频词上表现较好。

  Word2Vec的优势在于，它能够通过上下文信息捕捉到词语的语义，并且生成的向量维度较低（如100~300维），但依然能保留语义信息。

- **GloVe（Global Vectors for Word Representation）**：GloVe是斯坦福大学提出的一种词向量生成方法。它结合了共现矩阵的全局统计信息和Word2Vec的局部上下文信息，通过构建一个加权共现矩阵，并对矩阵进行优化训练，生成词向量。GloVe在某些情况下能够捕捉到更好的全局语义信息。

- **FastText**：FastText是Facebook提出的一种改进的Word2Vec模型。它将每个词拆解成多个n-gram（子词），再对这些子词进行训练。这种方法可以更好地处理拼写错误和未登录词（OOV，Out of Vocabulary），从而使模型在处理低频词和语言变体时表现更好。

- **BERT和Transformer系列模型**：与前面的静态词向量不同，BERT等Transformer模型能够根据上下文动态生成词向量。BERT通过深度双向编码器，从大规模语料中学习上下文信息，使得每个词的向量表示可以根据其上下文发生变化。例如，“bank”在“river bank”（河岸）和“financial bank”（银行）中会有不同的向量表示。这种动态特性使BERT在处理多义词和复杂语义时表现优异。

## 7.4. 词向量应用

词向量在NLP的诸多任务中都有广泛的应用，包括但不限于：

- **文本分类**：将词向量作为输入特征，可以用于情感分析、垃圾邮件检测等分类任务。
- **机器翻译**：词向量有助于模型理解源语言和目标语言之间的语义关系，从而提高翻译质量。
- **文本生成**：如自动摘要、对话生成等任务，可以利用词向量捕捉语义信息生成更流畅的文本。
- **语义相似度计算**：通过计算词向量之间的余弦相似度，判断两个词或句子之间的语义相似度。例如，可以用于推荐系统、搜索引擎中。

## 7.5. 词向量的优缺点

### 7.5.1 优点

- **高效表示**：相比于传统的词袋模型，词向量能够在低维空间中高效地表示词语，并捕捉到词语之间的语义关系。
- **语义信息**：词向量能够将语义相似的词语映射到相近的向量，从而帮助模型理解语义关系。
- **支持计算**：词向量具备数学计算性质，可以进行向量加减等操作，发现词语之间的隐含关系。

### 7.5.2 缺点

- **静态词向量的局限性**：如Word2Vec和GloVe，它们的词向量是静态的，对于同一个词在不同上下文中无法调整向量表示，从而在处理多义词时效果有限。
- **对语料依赖性强**：词向量的质量与训练语料的丰富度和多样性密切相关。若语料不充分，模型难以学习到高质量的词向量。
- **大模型计算开销高**：如BERT等基于Transformer的动态词向量模型，尽管效果更好，但计算成本较高，不适合所有场景。

## 7.6. 总结

词向量是自然语言处理中的基础工具，它将词语转化为可计算的向量，使得机器能够理解和操作语言的语义信息。从早期的Word2Vec、GloVe，到更复杂的FastText，再到BERT等动态词向量模型，词向量技术不断演进，应用场景也愈加广泛。随着NLP领域的进一步发展，词向量在语义理解、文本生成、机器翻译等任务中，依然是不可或缺的核心技术。

