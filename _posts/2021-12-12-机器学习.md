---
layout:     post
title:      机器学习课程笔记（Machine Learning）
subtitle:   逻辑回归（Logistic Regression）
date:       2021-12-12
author:     月月鸟
header-img: img/tag-bg.jpg
catalog: true
tags:
    - Machine Learning
---
这是一篇关于逻辑回归（Logistic Regression）的详细课程笔记，涵盖了其基本原理、特点、注意事项，以及实现逻辑回归模型的完整步骤。以下是笔记的要点：

## 逻辑回归的基本原理

### 1. 线性模型

逻辑回归首先使用线性模型计算输入变量的加权和：

\[
z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
\]

其中，\(x_i\) 是特征，\(w_i\) 是特征对应的权重，\(b\) 是偏置项。

### 2. 逻辑函数（Sigmoid函数）

线性模型的输出 \(z\) 被输入到逻辑函数中，将其转换为一个概率值：

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

这里，\(\sigma(z)\) 是事件发生的概率。

### 3. 阈值判断

通过设定一个阈值（通常为0.5），如果预测概率大于该阈值，则将该样本分类为正类（如1）；否则，分类为负类（如0）。

## 逻辑回归的特点

- **简单性**：逻辑回归计算简单，易于实现，解释性强。
- **效率高**：对于中小规模的数据集，逻辑回归的训练和预测速度较快。
- **应用广泛**：适用于需要输出概率的场景，如信用风险评估、广告点击率预测等。

## 注意事项

- **特征选择**：逻辑回归对输入特征的线性可分性有一定要求，因此在特征选择上需要谨慎。
- **多分类问题**：对于多分类问题，可以使用多项逻辑回归（Multinomial Logistic Regression）或通过一对一、一对多的方法将多分类问题转换为多个二分类问题。

## 实现步骤

### 1. 数据准备

加载数据集，分割训练集和测试集，并对特征进行标准化。

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

### 2. Sigmoid函数

实现Sigmoid函数。

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

### 3. 损失函数和梯度下降

定义逻辑回归的损失函数并使用梯度下降来更新参数 \(\theta\) 和 \(b\)。

#### 损失函数

\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
\]

#### 梯度更新公式

- 权重更新：

\[
\theta := \theta - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
\]

- 偏差更新：

\[
b := b - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})
\]

对应代码实现：

```python
def logistic_regression(X, y, learning_rate=0.01, num_iterations=10000):
    m, n = X.shape
    theta = np.zeros(n)
    bias = 0
    
    for i in range(num_iterations):
        linear_model = np.dot(X, theta) + bias
        y_pred = sigmoid(linear_model)
        
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum(y_pred - y)
        
        theta -= learning_rate * dw
        bias -= learning_rate * db
        
        if i % 1000 == 0:
            loss = - (1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
            print(f"Iteration {i}: Loss = {loss:.4f}")
    
    return theta, bias
```

### 4. 预测

通过训练好的参数进行预测。

```python
def predict(X, theta, bias):
    linear_model = np.dot(X, theta) + bias
    y_pred = sigmoid(linear_model)
    return [1 if i > 0.5 else 0 for i in y_pred]
```

### 5. 模型评估

使用测试数据集评估模型性能。

```python
theta, bias = logistic_regression(X_train, y_train, learning_rate=0.01, num_iterations=10000)

y_pred = predict(X_test, theta, bias)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("\nAccuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)
```

## 总结

逻辑回归是一种简单且有效的分类算法，尤其适用于需要概率输出的二分类任务。通过结合代码和公式，可以更好地理解逻辑回归的工作原理及其实际应用。