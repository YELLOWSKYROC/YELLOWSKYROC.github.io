---
layout:     post
title:      机器学习课程笔记（Machine Learning）
subtitle:   逻辑回归（Logistic Regression）
date:       2021-12-12
author:     月月鸟
header-img: img/tag-bg.jpg
catalog: true
tags:
    - Machine Learning
---

逻辑回归（Logistic Regression）是一种广泛使用的统计方法，主要用于二分类问题。虽然名字里有“回归”，但逻辑回归实际上是一种分类算法。它通过估计事件发生的概率来进行分类。逻辑回归的核心思想是使用逻辑函数（logistic function，也称为sigmoid函数）将线性回归的输出值映射到0到1之间，从而可以用来预测某个事件发生的概率。

### 逻辑回归的基本原理

1. **线性模型**  
   逻辑回归首先通过线性模型来计算输入变量的加权和，即：

   \[
   z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
   \]

   其中，\(x_i\) 是特征，\(w_i\) 是特征对应的权重，\(b\) 是偏置项。

2. **逻辑函数（Sigmoid函数）**  
   线性模型的输出 \(z\) 被输入到逻辑函数中，将其转换为一个概率值（范围在0到1之间）：

   \[
   \sigma(z) = \frac{1}{1 + e^{-z}}
   \]

   这里，\(\sigma(z)\) 是事件发生的概率。

3. **阈值判断**  
   通过设定一个阈值（通常为0.5），如果预测概率大于该阈值，则将该样本分类为正类（如1）；否则，分类为负类（如0）。

### 逻辑回归的特点

- **简单性**  
  逻辑回归计算简单，易于实现，解释性强。
- **效率高**  
  对于中小规模的数据集，逻辑回归的训练和预测速度较快。
- **应用广泛**  
  适用于需要输出概率的场景，如信用风险评估、广告点击率预测等。

### 注意事项

- **特征选择**  
  逻辑回归对输入特征的线性可分性有一定要求，因此在特征选择上需要谨慎。
- **多分类问题**  
  对于多分类问题，可以使用多项逻辑回归（Multinomial Logistic Regression）或通过一对一、一对多的方法将多分类问题转换为多个二分类问题。

总之，逻辑回归是一种简单且有效的分类算法，尤其适用于需要概率输出的二分类任务。

结合代码和公式，我们可以更直观地理解逻辑回归的实现过程。以下是详细步骤和代码示例：

### 1. 数据准备

在代码中，我们首先加载数据集，分割训练集和测试集，并对特征进行标准化。

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 加载乳腺癌数据集
data = load_breast_cancer()
X = data.data
y = data.target

# 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化数据
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

### 2. Sigmoid函数

Sigmoid函数将线性组合的结果映射为概率值，公式为：

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

对应的代码实现如下：

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

### 3. 损失函数和梯度下降

我们定义逻辑回归的损失函数和使用梯度下降来更新参数 \(\theta\) 和 \(b\)。

#### 损失函数

\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
\]

#### 梯度更新公式

- 权重更新：

\[
\theta := \theta - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
\]

- 偏差更新：

\[
b := b - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})
\]

对应代码实现：

```python
def logistic_regression(X, y, learning_rate=0.01, num_iterations=10000):
    m, n = X.shape
    theta = np.zeros(n)
    bias = 0
    
    for i in range(num_iterations):
        linear_model = np.dot(X, theta) + bias
        y_pred = sigmoid(linear_model)
        
        # 计算梯度
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum(y_pred - y)
        
        # 更新参数
        theta -= learning_rate * dw
        bias -= learning_rate * db
        
        # 每1000次迭代输出损失
        if i % 1000 == 0:
            loss = - (1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
            print(f"Iteration {i}: Loss = {loss:.4f}")
    
    return theta, bias
```

### 4. 预测

通过训练好的参数进行预测：

\[
h_\theta(x) = \sigma(X \cdot \theta + b)
\]

如果 \(h_\theta(x) \geq 0.5\)，则预测为正类（1），否则为负类（0）。

```python
def predict(X, theta, bias):
    linear_model = np.dot(X, theta) + bias
    y_pred = sigmoid(linear_model)
    return [1 if i > 0.5 else 0 for i in y_pred]
```

### 5. 模型评估

使用测试数据集评估模型性能：

```python
# 训练逻辑回归模型
theta, bias = logistic_regression(X_train, y_train, learning_rate=0.01, num_iterations=10000)

# 进行预测
y_pred = predict(X_test, theta, bias)

# 评估模型性能
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# 打印结果
print("\nAccuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)
```

### 总结

- **线性模型**：通过特征和权重的线性组合得到结果。
- **Sigmoid函数**：将线性结果映射为概率。
- **损失函数**：使用交叉熵损失来评估预测误差。
- **梯度下降**：通过迭代更新权重和偏差来最小化损失。
- **预测和评估**：使用训练好的模型进行预测并评估其性能。

通过结合代码和公式，我们可以更好地理解逻辑回归的工作原理，以及如何在实际中实现和应用它。

