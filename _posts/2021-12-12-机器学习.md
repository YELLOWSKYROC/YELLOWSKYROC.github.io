---
layout:     post
title:      机器学习课程笔记（Machine Learning）
subtitle:   逻辑回归（Logistic Regression）
date:       2021-12-12
author:     月月鸟
header-img: img/tag-bg.jpg
catalog: true
tags:
    - Machine Learning
---


介绍介绍机器学习中逻辑回归（Logistic Regression）的基本原理、特点、注意事项，以及实现逻辑回归模型的完整步骤。

# 1. 逻辑回归的基本原理

## 1.1 线性模型

逻辑回归首先使用线性模型计算输入变量的加权和：

$$
z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b = w^Tx+b
$$

其中，\(x_i\) 是特征，\(w_i\) 是特征对应的权重，\(b\) 是偏置项。

### 1.2 逻辑函数（Sigmoid函数）

线性模型的输出 \(z\) 被输入到逻辑函数中，将其转换为一个概率值：

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^Tx+b)}}
$$

这里，\(\sigma(z)\) 表示事件发生的概率。

### 1.3 阈值判断

通过设定一个阈值（通常为0.5），如果预测概率大于该阈值，则将样本分类为正类（如1）；否则，分类为负类（如0）。


## 2. 实现步骤

我使用**breast_cancer**数据集来做一个简单的实现来帮助我们理解。乳腺癌数据集通常指的是用来进行乳腺癌诊断的一个数据集，广泛用于机器学习和统计分析。最著名的乳腺癌数据集之一是“威斯康星乳腺癌数据集”（Wisconsin Breast Cancer Dataset）

### 2.1 数据准备

加载数据集，分割训练集和测试集，并对特征进行标准化。

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 加载乳腺癌数据集
data = load_breast_cancer()
X = data.data  # 特征数据
y = data.target  # 目标标签（恶性或良性）

# 将数据集分为训练集和测试集，80%用于训练，20%用于测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()  # 标准化特征数据，确保每个特征的均值为0，方差为1
X_train = scaler.fit_transform(X_train)  # 计算训练集的均值和标准差，并标准化训练集
X_test = scaler.transform(X_test)  # 使用训练集的均值和标准差来标准化测试集
```

### 2.2 Sigmoid函数

实现Sigmoid函数。

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

### 2.3 损失函数和梯度下降

定义逻辑回归的损失函数并使用梯度下降来更新参数 \(w\) 和 \(b\)。

### 2.3.1 损失函数

计算所有样本的平均交叉熵损失。

$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_w(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_w(x^{(i)})) \right]
$$

### 2.3.2 梯度更新公式
梯度下降通过调整权重 \(w\) 来最小化损失函数。类似地，调整偏差 \(b\) 以最小化损失函数。
- **权重更新**：

$$
w := w - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
$$

- **偏差更新**：

$$
b := b - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})
$$

代码实现：

```python
def logistic_regression(X, y, learning_rate=0.01, num_iterations=10000):
    """
    执行逻辑回归训练
    
    参数:
    X -- 特征矩阵 (m x n)
    y -- 标签向量 (m,)
    learning_rate -- 学习率
    num_iterations -- 迭代次数

    返回:
    theta -- 学习得到的权重
    bias -- 学习得到的偏差
    """
    m, n = X.shape  # 获取样本数 m 和特征数 n
    theta = np.zeros(n)  # 初始化权重为零
    bias = 0  # 初始化偏差为零
    
    for i in range(num_iterations):
        # 计算线性模型输出
        linear_model = np.dot(X, theta) + bias
        # 计算预测的概率值
        y_pred = sigmoid(linear_model)
        
        # 计算权重的梯度
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        # 计算偏差的梯度
        db = (1 / m) * np.sum(y_pred - y)
        
        # 更新权重和偏差
        theta -= learning_rate * dw
        bias -= learning_rate * db
        
        # 每1000次迭代打印一次损失值
        if i % 1000 == 0:
            # 计算当前损失
            loss = - (1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
            print(f"Iteration {i}: Loss = {loss:.4f}")
    
    return theta, bias
```

### 2.4 预测

然后通过训练好的参数进行预测。

```python
def predict(X, theta, bias):
    """
    使用逻辑回归模型进行预测
    
    参数:
    X -- 特征矩阵 (m x n)
    theta -- 权重向量 (n,)
    bias -- 偏差标量
    
    返回:
    y_pred -- 预测标签，列表形式 (m,)
    """
    # 计算线性模型输出
    linear_model = np.dot(X, theta) + bias
    # 计算预测的概率值
    y_pred = sigmoid(linear_model)
    
    # 根据预测概率值生成二分类标签（0或1），阈值为0.5
    return [1 if i > 0.5 else 0 for i in y_pred]
```

### 2.5 模型评估

使用测试数据集评估模型性能。

```python
theta, bias = logistic_regression(X_train, y_train, learning_rate=0.01, num_iterations=10000)
y_pred = predict(X_test, theta, bias)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("\nAccuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)
```

## 3. 优缺点
逻辑回归作为一种广泛使用的分类算法，具有一些显著的优点和缺点。下面是它的详细分析：

### 3.1 优点

1. **简单易用**：
   - 逻辑回归算法易于理解和实现，适合入门级学习者。

2. **解释性强**：
   - 模型的系数可以直接解释为特征对结果的影响，提供了良好的可解释性。

3. **概率输出**：
   - 可以输出类别的概率，使得结果更具信息性，可以帮助决策过程。

4. **计算效率高**：
   - 算法复杂度低，适用于大规模数据集。

5. **鲁棒性强**：
   - 对少量异常值不敏感，因为使用了对数损失函数。

6. **适用于二分类和多分类任务**：
   - 可以通过简单的扩展处理多分类问题，如一对多策略或使用多项逻辑回归。

### 3.2 缺点

1. **线性假设**：
   - 假设特征与对数几率之间是线性关系，对于复杂的非线性关系的处理能力有限。

2. **特征工程要求高**：
   - 需要对数据进行适当的预处理和特征选择，敏感于不相关或共线性特征。

3. **对数据规模敏感**：
   - 在特征数量远多于样本数量的情况下，可能导致过拟合。

4. **处理类别不平衡数据能力有限**：
   - 在类别不平衡的情况下，模型可能偏向于预测多数类。

5. **不适用于大数据的无约束问题**：
   - 在某些情况下，可能需要引入正则化来防止过拟合，而正则化需要仔细选择超参数。

6. **无法自动捕获复杂关系**：
   - 无法像神经网络等复杂模型那样自动捕获非线性关系和特征交互，需要人工构造特征。

逻辑回归在需要简单且可解释的模型时非常有效，但在处理复杂、非线性的数据时可能需要其他更为复杂的模型。根据具体任务的需求和数据特性选择合适的模型是非常重要的。


## 6. 多分类扩展
逻辑回归也可以通过 **一对多（One-vs-Rest）或多项逻辑回归（Multinomial Logistic Regression** 扩展到多分类任务。
