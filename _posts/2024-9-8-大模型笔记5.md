---
layout:     post
title:      大模型笔记（LLM）
subtitle:   大语言模型基础5
date:       2024-9-8
author:     月月鸟
header-img: img/LLM.jpg
catalog: true
tags:
    - LLM
---

# 1. 大语言模型评估

## 1.1. 大模型的评测方法

评测大语言模型的性能通常包括**自动评测**和**人工评测**两种方法：

- **自动评测**：基于计算机算法和自动生成的指标，自动评测可以快速且高效地评估模型的表现。例如，使用准确率、F1 得分、BLEU 等标准化指标来评估模型在语言生成、分类和翻译任务上的表现。自动评测的优点是效率高、成本低、易于大规模应用，但其缺点是往往无法捕捉到模型生成文本中的深层语义和细微差别。

- **人工评测**：由人类专家进行主观评价。评测人员会针对模型生成的内容进行细致的评估，包括语言的流畅度、准确性、上下文理解、信息完整性和逻辑一致性等。人工评测能够提供更深入的反馈，适合评估模型在复杂语言任务中的表现，例如对话生成的自然性和文本的情感表达等。虽然人工评测的结果更具解释性，但它通常耗时且成本较高，难以大规模应用。

在实际应用中，通常会结合这两种方法，以便更全面地了解模型的优缺点。

## 1.2. 大模型的 Honest 原则如何实现？

大语言模型在实际应用中需要遵循“**Helpful, Honest, Harmless**”三大原则（即帮助用户、保持诚实、避免伤害）。其中，“Honest”（诚实）的原则要求模型在生成回答时能够区分已知信息与未知信息，不要编造答案。

为了使模型更好地遵循“Honest”原则，可以通过有针对性地设计训练样本来强化这一能力，具体策略包括：

- **知识问答训练**：在微调阶段，可以构造一个知识问答类的训练集，其中包含模型不知道答案时应如何回应的样本。例如，模型在遇到不确定的问题时，应该回答“我不知道”或“我无法回答此问题”，而不是编造答案。通过这样的样本训练，可以使模型学会在信息不足时保持诚实。

- **阅读理解训练**：设计阅读理解类的训练数据，要求模型在阅读特定文本后回答问题。如果问题的答案不在阅读材料中，模型应学习到正确的回应方式，即承认未能找到答案。这种训练能够有效地帮助模型区分“已知信息”和“未知信息”，避免无根据地推测。

- **对抗训练**：通过设计一些可能诱导模型编造答案的问题，例如“X的研究结果是什么”，并将正确的回答标记为“我对此没有足够的信息”，从而强化模型对诚实回答的偏好。

通过这些训练方法，模型逐渐学会如何在面对未知或不确定的信息时，选择更诚实的回答方式。

## 1.3. 如何衡量大模型的水平？

在评估大语言模型（LLMs）的能力时，需要选择合适的任务和领域，以全面展示模型的优势和局限性。可以将现有的任务划分为以下7个类别，以衡量模型的性能：

1. **自然语言处理（NLP）**：包括自然语言理解（NLU）、自然语言生成（NLG）、推理能力以及多语言支持等任务。这类任务能够衡量模型的语言理解深度和生成能力。

2. **鲁棒性、伦理、偏见和真实性**：评估模型在面对对抗性输入时的鲁棒性，以及是否存在伦理问题或语言偏见。此外，还包括模型生成的内容是否准确、可信。

3. **医学应用**：评估模型在医学领域的应用能力，例如医学问答、医学考试、医学教育和医疗辅助等。这类任务能够展示模型在专业领域的知识掌握程度和应用能力。

4. **社会科学**：包括心理学、社会学等领域的知识理解和应用，评估模型在社会科学知识上的准确性和分析能力。

5. **自然科学与工程**：包括数学、物理、化学等自然科学以及工程学科，测试模型在这些领域内的逻辑推理和问题解决能力。

6. **代理应用**：评估模型在作为智能助手、对话代理等角色时的表现能力，包括对话的流畅性和多轮对话上下文的理解。

7. **其他应用**：包括模型在创意写作、文化内容生成等非专业性任务中的表现。

通过这些类别，可以更全面地了解模型的强项和短板。

## 1.4. 大模型的评估方法有哪些？

大模型的评估方法主要包括以下三类：

1. **直接评估指标**：这是人工智能领域传统而广泛使用的方法，包括准确率（Accuracy）、F1 得分、BLEU 分数、ROUGE 等。这种方法通过与参考答案的对比来评估模型的表现，适用于机器翻译、文本分类等任务。

2. **间接或分解的启发式方法**：利用更小的模型或微调过的模型来评估主模型生成的答案。例如，可以用一个微调过的模型来评估主模型的生成文本是否符合某种预期或标准。这种方法适用于复杂任务的细化评估，特别是当直接评估指标不足以衡量模型性能时。

3. **基于模型的评估**：在这种方法中，模型本身提供评估分数或结果。例如，通过将模型生成的文本再次输入到模型中，以获得对生成文本的评价。这种方法的灵活性较高，但也会引入评估过程中的随机因素，需要谨慎使用。

## 1.5. 大模型评估工具有哪些？

以下是几种常用的大模型评估工具：

- **ChatbotArena**：借鉴游戏排位赛的机制，用户对模型进行两两对比评估，适用于对话生成类模型的主观评估。

- **SuperCLUE**：中文通用大模型的综合评测基准，旨在通过全自动化的方式评估模型在不同中文任务上的表现。

- **C-Eval**：使用1.4万道涵盖52个学科的选择题，专门用于评估模型的中文理解和生成能力。

- **FlagEval**：采用“能力—任务—指标”三维评测框架，系统化地评估模型在不同任务和能力维度上的表现。

这些工具各具特色，适用于不同的应用场景，有助于全面、系统地评估大模型的能力。

---

# 2. LLM幻觉

### 1. 什么是大模型幻觉？

在大语言模型（LLM）的背景下，**幻觉**（Hallucination）是指模型生成的文本表面上看起来流畅自然，但实际上与事实不符，甚至包含错误信息的现象。它表现为模型在回答问题或生成内容时，“一本正经地胡说八道”。幻觉现象影响了模型输出的准确性和可信度，是在自然语言生成和其他复杂任务中面临的一个重大挑战。

本文将深入探讨大型语言模型（LLMs）中的幻觉问题，并介绍应对这种现象的一些常见方法。

### 2. 为什么需要解决 LLM 的幻觉问题？

解决 LLM 的幻觉问题非常重要，因为它可能带来严重的后果，包括：

- **传播错误信息**：模型的幻觉可能会生成不准确的内容，特别是在信息传播场景中，错误的信息可能被广泛引用和扩散。
- **侵犯隐私**：幻觉可能导致模型生成不真实的、但看似可信的私人信息，从而带来隐私泄露风险。
- **医疗应用中的潜在风险**：例如，在医疗场景中，如果模型生成的患者报告中包含虚假的或不准确的信息，可能会导致误诊、治疗错误，甚至对患者的生命构成威胁。

因此，幻觉问题直接影响到模型的**可靠性**和**可信度**，解决这一问题对于保障模型在实际应用中的安全性和有效性至关重要。

### 3. 幻觉一定是有害的吗？

幻觉并不总是有害的。在某些需要**创造力**或**灵感**的场景中，它甚至可能带来积极的效果。例如：

- **写作和创意生成**：在编写小说、电影剧情或创意文案时，模型生成的幻觉可能包含丰富的想象力和意外的灵感，使得内容更加生动有趣。
- **艺术创作**：模型生成的看似“离谱”的内容可能正是艺术创作中所需的突破点，能够激发人们的想象力。

因此，对幻觉的容忍度取决于应用场景。在创意场景中，适当的“幻觉”反而可能是模型的一种优势，但在信息敏感的领域，如法律、医疗等，则需要严格控制。

### 4. 幻觉有哪些不同类型？

幻觉大致可以分为两类：**内在幻觉**和**外在幻觉**。

- **内在幻觉**：模型生成的内容与源内容存在逻辑上的矛盾。例如，在对话过程中，模型前后生成的回答自相矛盾。
- **外在幻觉**：模型生成的内容在外部无法被验证。也就是说，生成的内容既没有得到源信息的支持，也不能被其否定。例如，当模型回答一个问题时，生成的信息无法通过外部知识库或真实数据来确认其准确性。

### 5. 为什么 LLM 会产生幻觉？

研究表明，LLM 产生幻觉的原因是多方面的，包括以下几点：

- **源与目标的差异**：在训练过程中，如果模型的源数据与目标数据之间存在差异，模型可能会生成偏离原始内容的输出。这种差异可能是由于数据收集过程中的不一致性，也可能是出于数据多样化的设计。

- **无意识的源-目标差异**：有时源数据和目标数据之间的差异并非刻意制造。例如，来自不同新闻网站的关于同一事件的报道，可能会包含不同的细节信息。

- **有意识的源-目标差异**：某些任务本身并不要求生成内容与源完全一致。例如，在生成多样化对话回复时，模型需要生成更具创意的内容，而不是机械地复制源信息。

- **训练数据的重复性**：如果训练数据中某些短语或语句出现频率过高，模型在生成时可能会过于偏向这些常见短语，从而引发幻觉现象。

- **数据噪声的影响**：训练数据中的噪声，如拼写错误、逻辑不一致或错误信息，可能导致模型学到这些错误的模式，从而在生成时出现幻觉。

- **解码过程中的随机性**：在生成文本时，使用如 Top-k 采样、Top-p 方法或温度调节等策略会引入随机性。这些策略在增强文本多样性的同时，也可能增加幻觉的发生率，因为模型没有总是选择最有可能的输出。

- **模型的参数知识偏向**：模型在预训练阶段积累了大量的知识，在实际应用中可能更依赖这些已有知识，而忽略了实时上下文中的关键信息。

- **训练与推理的差异**：在训练中，模型通过真实数据学习如何预测下一个词汇，而在推理时，它是基于先前生成的内容来预测后续文本。这种方式上的差异，特别是在处理长文本时，容易导致生成内容的偏离和幻觉。

### 6. 如何度量幻觉？

度量幻觉的最直接、可靠的方法是人工评估，但由于成本高且难以大规模应用，自动化评估方法逐渐被采用，包括以下几种：

- **命名实体误差**：命名实体（NEs）在事实性描述中占据重要位置。通过比较模型生成文本中的命名实体与参考资料中实体的一致性，检测模型是否生成了虚假的信息。

- **蕴含率**：计算生成文本中与参考文本之间的语句蕴含关系，评估生成文本是否符合参考文本中的信息。这可以通过自然语言推理（NLI）模型来实现。

- **基于模型的评估**：利用其他小模型或微调模型对主模型生成的内容进行分析，尤其是在复杂的语法或语义变化情况下。

- **利用问答系统**：给定模型生成的内容，生成一组问题，然后使用原始参考文本回答这些问题，通过对比生成内容与参考答案的相似度来评估幻觉的程度。

- **信息提取系统**：将生成的内容和参考文本转化为关系元组（例如 <主体，关系，对象>），并对比两者之间的关系是否一致。

### 7. 如何缓解 LLM 幻觉？

缓解 LLM 幻觉的方法包括：

- **提高数据质量**：通过构建高质量、无噪声的数据集，减少模型学习错误模式的机会。然而，验证和清理大量的文本数据集是非常困难的。

- **利用外部知识库**：在生成过程中调用外部知识库或搜索引擎来核实生成内容的准确性，从而减少幻觉。

- **优化解码策略**：调整生成文本时的解码策略，如降低 Top-p 参数的值，减少生成内容的随机性，或采用专门的事实增强解码算法。

- **多次采样验证一致性**：如使用 SelfCheckGPT 的方法，采样多次输出并检查不同生成结果之间的信息一致性，以此来检测和缓解幻觉。

### 8. LLMs 什么时候最容易产生幻觉？

LLMs 最容易在以下场景中产生幻觉：

- **数值混淆**：处理与数字、日期、统计数据等相关的文本时，模型容易出现错误。

- **处理长文本**：在需要解读长期依赖关系的任务中，例如文档摘要或长对话历史，模型更容易生成自相矛盾的内容。

- **逻辑推理不足**：在推理能力不足或对源文本理解不准确时，模型可能生成错误结论。

- **上下文与内置知识冲突**：当模型的预训练知识与给定上下文信息不一致时，模型可能会偏向输出预训练知识，从而导致幻觉。

- **错误的上下文信息**：当输入问题本身包含错误或悖论（如“为什么氦的原子序数是1？”）时，模型可能无法识别这些错误，从而产生幻觉。

### 9. 幻觉的分类

幻觉可以进一步细分为以下几类：

- **事实性问题（Factuality）**：
  - **事实性错误**：模型生成的内容与真实世界事实不一致。
  - **事实性虚构**：模型生成的内容无法在真实世界中得到验证。

- **忠诚度问题（Faithfulness）**：
  - **违背指令**：生成内容没有遵循用户的指令或要求。
  - **违背上文**：生成内容与上下文或源文本之间存在不一致。

- **自我矛盾（Self-Contradiction）**：模型在推理过程中前后不一致，例如在连续推理步骤中得出相互矛盾的结论。

### 10. 幻觉检测和缓解方法

**检测方法**：

- **外部检索增强**：通过搜索引擎或知识库来核实模型生成内容

的正确性。
- **模型回答的不确定性**：通过测量回答的熵值或多次采样的结果一致性来判断内容的可靠性。
- **忠诚度检测**：包括实体匹配、知识对比和基于自然语言推理的模型检测。

**缓解方法**：

- **改进数据源**：提高训练数据的质量，减少错误和偏见。
- **使用外部检索机制**：通过将模型与外部知识库结合，增强生成内容的事实性。
- **优化解码策略**：使用更加稳健的解码算法，如**factual-nucleus**或**SelfCheckGPT**，来提升生成内容的准确性。
- **模型对齐和微调**：在微调过程中允许模型表达不确定性，并调整 RLHF 策略以避免模型过度迎合用户。

通过这些方法，可以有效地减轻 LLMs 的幻觉问题，提高模型在复杂任务中的可靠性和应用价值。




