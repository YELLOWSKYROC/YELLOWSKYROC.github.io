---
layout:     post
title:      ResNet（Residual Neural Network）
subtitle:   深度学习
date:       2022-07-30
author:     月月鸟
header-img: img/text-classification.png
catalog: true
tags:
    -  Deep Learning
---

ResNet（Residual Neural Network）是由何凯明等人于2015年提出的一种深度卷积神经网络，因其提出的“残差连接”（Residual Connections）或称为“跳跃连接”（Skip Connections）而闻名。ResNet 解决了在深层神经网络中常见的“梯度消失”问题，并允许模型在极大深度下仍然有效地进行训练。它的出现使得网络可以达到数百甚至上千层的深度，同时保持较好的性能和易于训练。

### ResNet 的核心思想

#### 1. **深度神经网络的瓶颈：梯度消失问题**
在传统的深层神经网络中，随着层数的增加，梯度消失或爆炸问题变得更加严重。梯度消失会导致网络中的参数更新变得极其缓慢，最终网络无法有效训练。研究发现，超过一定深度后，网络的训练误差开始增加，导致性能下降。这一问题极大地限制了深度网络的进一步发展。

#### 2. **残差学习（Residual Learning）**
ResNet 的创新之处在于引入了残差学习的思想。通常情况下，网络希望直接学习输入到输出的映射 \( H(x) \)，但在 ResNet 中，网络不是直接学习该映射，而是学习输入和输出之间的“残差” \( F(x) = H(x) - x \)。这样，模型实际学习的是：如何将输入 \( x \) 转化为 \( F(x) \) 再加上 \( x \)，即 \( H(x) = F(x) + x \)。

这种结构的直观解释是，网络可以更容易地学习那些接近恒等映射的函数。残差块允许网络通过简单的恒等变换跳过不必要的层，因此它不会因网络深度过大而导致梯度消失。

#### 3. **残差块（Residual Block）**
ResNet 的基础构建模块是“残差块”。在残差块中，输入 \( x \) 直接跳过一个或多个卷积层，并与该层的输出相加。这种直接的跳跃连接使得信息可以跨层传播，避免了深度网络中过度学习不必要的特征。

残差块的数学公式为：
\[
H(x) = F(x) + x
\]
其中：
- \( x \) 是输入
- \( F(x) \) 是经过若干层非线性变换的输出
- \( H(x) \) 是残差块的输出

这种结构使得模型不仅能学习复杂的非线性映射，还可以自动学习保持输入不变（恒等映射），这在浅层网络上尤其重要。

### ResNet 的结构

ResNet 是由多个残差块堆叠而成，不同版本的 ResNet（如 ResNet-18、ResNet-34、ResNet-50、ResNet-101、ResNet-152）具有不同的层数，但它们的基本构造相同。常见的 ResNet 结构如下：

#### 1. **输入层**：
- 通常是一个 7x7 的大卷积核和 64 个通道的卷积操作，步幅为 2。
- 接着是一个 3x3 的最大池化层，同样步幅为 2，用于下采样。

#### 2. **残差块（Residual Blocks）**：
- 不同的版本堆叠不同数量的残差块。每个残差块包含两个或三个卷积层。对于 ResNet-50 及更深的模型，使用了带瓶颈的残差块，每个块包含三个卷积层（1x1, 3x3, 1x1）。
- 每个卷积层后面都接有批归一化（Batch Normalization）和 ReLU 激活函数。
- 当需要改变特征图的尺寸时（如下采样），可以通过步幅为 2 的卷积实现，同时跳跃连接的输入也会通过 1x1 的卷积进行维度调整。

#### 3. **全局平均池化（Global Average Pooling）**：
- 通过全局平均池化层将特征图的每个通道压缩为一个标量，从而减少参数量。

#### 4. **全连接层**：
- 最后连接一个全连接层，用于最终的分类。

### 不同版本的 ResNet
ResNet 提供了不同深度的模型，层数越多模型的能力越强，但计算复杂度也随之增加。常见的版本包括：

- **ResNet-18**：包含 18 层网络，使用两个卷积层的残差块。
- **ResNet-34**：较深的网络，包含 34 层，同样使用两个卷积层的残差块。
- **ResNet-50**：使用带瓶颈的残差块，每个块有三个卷积层，网络总共包含 50 层。
- **ResNet-101**：同样使用瓶颈块，总共 101 层。
- **ResNet-152**：更深的版本，包含 152 层。

表中列出了 ResNet 的不同版本和对应的残差块数量：

| 模型     | 残差块数量 | 总层数 | 结构                         |
|----------|------------|--------|------------------------------|
| ResNet-18 | 2 层残差块 | 18 层  | 2 层卷积，每个残差块           |
| ResNet-34 | 2 层残差块 | 34 层  | 2 层卷积，每个残差块           |
| ResNet-50 | 3 层瓶颈块 | 50 层  | 1x1 卷积，3x3 卷积，1x1 卷积  |
| ResNet-101| 3 层瓶颈块 | 101 层 | 1x1 卷积，3x3 卷积，1x1 卷积  |
| ResNet-152| 3 层瓶颈块 | 152 层 | 1x1 卷积，3x3 卷积，1x1 卷积  |

### ResNet 的改进

1. **瓶颈结构（Bottleneck Block）**：
   在较深的版本（如 ResNet-50、ResNet-101、ResNet-152）中，ResNet 使用了瓶颈结构来减少计算量。瓶颈结构使用 1x1 卷积来降低通道维度，然后使用 3x3 卷积进行空间特征提取，最后再通过 1x1 卷积恢复原来的通道维度。这样可以有效减少计算复杂度，而不牺牲性能。

2. **Batch Normalization（批归一化）**：
   批归一化层被应用于每个卷积层后，帮助加速模型的收敛，并且稳定深层模型的训练。

### ResNet 的优势

1. **有效缓解梯度消失问题**：
   残差连接允许梯度更好地在网络中传播，解决了传统深层神经网络中梯度消失的问题。这使得 ResNet 可以训练非常深的网络。

2. **模型深度显著增加性能**：
   ResNet 的结构使得模型可以在不增加复杂度的情况下进一步堆叠更多层，提高网络的表征能力。深层网络在图像分类任务上取得了显著的性能提升。

3. **易于优化和泛化**：
   残差结构的引入减少了深度网络的训练难度，提高了模型的泛化性能，在 ImageNet、CIFAR 等多个数据集上都取得了卓越的性能。

### ResNet 的应用

由于 ResNet 强大的表征能力和较好的训练稳定性，它广泛应用于计算机视觉领域的各种任务，包括但不限于：

1. **图像分类**：ResNet 在 ImageNet 比赛中取得了极大的成功，并且成为了许多图像分类任务的基础网络。
2. **目标检测**：ResNet 经常用作 Faster R-CNN、YOLO 等目标检测算法的特征提取骨干网络。
3. **语义分割**：ResNet 作为特征提取器在 UNet 和 DeepLab 等语义分割模型中表现出色。
4. **其他领域**：ResNet 也被迁移到其他领域，例如医学图像分析、视频分类等。

### 简单计算例子

为了帮助理解 ResNet 的工作原理，下面是一个简单的 ResNet 残差块的计算示例。我们以一个两层卷积的残差块为例，展示它是如何通过跳跃连接（skip connection）来完成计算的。

### 假设的参数：
- 输入 \( x \)：一个 3 通道的 64x64 图像，形状为 \( (3, 64, 64) \)。
- 第一层卷积：3 个 3x3 卷积核，输出 3 通道的特征图，步幅为 1，填充为 1（保持空间维度不变），ReLU 激活。
- 第二层卷积：相同的设置，即 3 个 3x3 卷积核，输出 3 通道的特征图，步幅为 1，填充为 1，ReLU 激活。
- 跳跃连接：直接将输入 \( x \) 加到卷积结果上。

### 计算步骤：

1. **输入数据**：
   输入 \( x \) 形状为 \( (3, 64, 64) \)，表示 3 通道的 64x64 图像。

2. **第一层卷积计算**：
   假设卷积核大小为 \( 3 \times 3 \)，步幅为 1，填充为 1（确保输出的空间维度与输入相同）。经过第一层卷积，输出的特征图形状仍然为 \( (3, 64, 64) \)，加上 ReLU 激活函数：
   \[
   y_1 = ReLU(W_1 * x + b_1)
   \]
   其中 \( W_1 \) 是第一层的卷积权重，\( * \) 表示卷积操作，\( b_1 \) 是偏置，\( y_1 \) 是第一层卷积的输出。

3. **第二层卷积计算**：
   再次经过相同参数的第二层卷积，输出特征图的形状仍然为 \( (3, 64, 64) \)，加上 ReLU 激活函数：
   \[
   y_2 = ReLU(W_2 * y_1 + b_2)
   \]
   其中 \( W_2 \) 是第二层的卷积权重，\( b_2 \) 是偏置，\( y_2 \) 是第二层卷积的输出。

4. **残差连接（跳跃连接）**：
   通过跳跃连接，将输入 \( x \) 直接与卷积后的输出 \( y_2 \) 相加：
   \[
   out = y_2 + x
   \]
   这就是 ResNet 的核心思想：通过这种加法，模型可以更容易学习到恒等映射，并且缓解梯度消失问题。

5. **激活函数**：
   最后经过 ReLU 激活，得到残差块的最终输出：
   \[
   output = ReLU(out)
   \]
   最终输出的形状仍然为 \( (3, 64, 64) \)。

### 总结：
- 输入图像 \( x \) 为 \( (3, 64, 64) \)。
- 两次卷积后，通过跳跃连接将输入直接与卷积结果相加。
- 最后的输出形状保持不变，仍然是 \( (3, 64, 64) \)。

### 总结

ResNet 是一种革命性的深度卷积神经网络架构，它通过残差学习极大地缓解了深层网络的梯度消失问题，使得深层网络的训练更加稳定和高效。它的残差连接结构简单而有效，使得 ResNet 成为许多计算机视觉任务的标准选择。
