---
layout:     post
title:      机器学习中的激活函数(Activation Function)
subtitle:   机器学习课程
date:       2022-7-5
author:     月月鸟
header-img: img/text-classification.png
catalog: true
tags:
    - Machine Learning
---

激活函数（Activation Function）是神经网络中非常重要的组成部分，其主要作用是为每个神经元引入非线性特性，使网络能够学习和表示复杂的数据模式。激活函数的选择对神经网络的性能、收敛速度、以及训练的有效性有很大影响。

![](https://media.licdn.com/dms/image/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&v=beta&t=gFWxErTLLWBc6iRWDxCBRxkdJ7ob24cmjWZAOuKN9o4)


# 1. Sigmoid 函数（S型函数）
**定义：**
$$ f(x) = \frac{1}{1 + e^{-x}} $$

**特点：**
- 输出范围在 \(0\) 到 \(1\) 之间。
- 常用于二分类问题的输出层。
- 函数的曲线呈现 S 型，平滑且连续。

**优点：**
- 值域有限，适用于概率预测。
- 能够将较大的输入值压缩到一个较小的范围内。

**缺点：**
- **梯度消失问题**：对于大正或大负的输入，梯度会非常接近于零，导致反向传播时更新速度极慢。
- **非零中心输出**：Sigmoid 的输出范围是正值，这可能导致梯度更新不均衡。

---

# 2. Tanh 函数（双曲正切函数）
**定义：**
$$ f(x) = \frac{2}{1 + e^{-2x}} - 1 $$
或
$$ f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

**特点：**
- 输出范围在 \(-1\) 到 \(1\) 之间。
- 常用于隐藏层的激活函数。

**优点：**
- **零中心输出**：输出范围在负数和正数之间，梯度更新更加均衡。
- 在许多情况下比 Sigmoid 函数表现更好。

**缺点：**
- 也存在**梯度消失问题**，特别是对于极端输入时，Tanh 的梯度接近于零，导致网络学习变得非常缓慢。

---

# 3. ReLU 函数（修正线性单元）
**定义：**
$$ f(x) = \max(0, x) $$

**特点：**
- 非线性函数，输出为正数的线性部分或零。

**优点：**
- **计算效率高**：ReLU 的计算非常简单且快速，仅需判断输入是否为正。
- **解决梯度消失问题**：在正区间梯度为1，不存在梯度消失问题，因此可以加速模型的训练。
- **稀疏激活**：许多神经元的输出为零，这种稀疏性可以提高网络的泛化能力。

**缺点：**
- **死亡 ReLU 问题**：当输入为负数时，梯度为零，导致某些神经元在训练过程中永远不会被激活。
- **非平滑性**：在 \(x = 0\) 处的不可微性有时会影响优化过程。

---

# 4. Leaky ReLU 函数（带泄漏的修正线性单元）
**定义：**
$$ f(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}
$$
其中 \(\alpha\) 是一个很小的常数（例如 0.01）。

**特点：**
- 类似于 ReLU，但在 \(x \leq 0\) 时会允许一个小的负梯度。

**优点：**
- 减少了死亡 ReLU 问题，负值区域也可以传递梯度，保持学习能力。

**缺点：**
- 需要手动设置 \(\alpha\) 参数，且对于不同任务该参数可能需要调优。

---

# 5. ELU 函数（指数线性单元）
**定义：**
$$ f(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \leq 0
\end{cases}
$$

**特点：**
- 在正区域与 ReLU 类似，但负区域会逐渐趋向于一个负常数而不是零。

**优点：**
- 负区间的输出不会像 ReLU 那样截断为零，能更好地避免死亡神经元问题。
- 更平滑的梯度变化，提升模型的收敛速度。

**缺点：**
- 相对复杂的计算，特别是在负值区域的指数运算上。

---

# 6. Softmax 函数
**定义：**
$$ f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}} $$

**特点：**
- 将一个向量中的每个元素转换为 0 到 1 之间的概率值，且总和为 1。
- 常用于多分类问题的输出层。

**优点：**
- 适用于多类别分类，能够将输出转换为概率分布。
- 能够处理多个类之间的竞争关系。

**缺点：**
- 当某个输出非常大时，容易导致其他输出的梯度非常小。

---

# 7. Swish 函数
**定义：**
$$ f(x) = x \cdot \text{sigmoid}(x) = \frac{x}{1 + e^{-x}} $$

**特点：**
- 是由 Google 提出的一个新的激活函数，结合了 ReLU 和 Sigmoid 的优势。

**优点：**
- 在实践中，Swish 通常表现得比 ReLU 和 Sigmoid 更好，尤其是在深层神经网络中。
- 平滑的曲线能够带来更好的梯度流动。

**缺点：**
- 相较于 ReLU，Swish 的计算量较大。

---

# 8. GELU 函数定义

GELU（**Gaussian Error Linear Unit**，高斯误差线性单元）是近年来广泛应用于深度学习中的一种激活函数，尤其在像 BERT 这样的自然语言处理模型中表现出色。GELU 函数结合了 ReLU 和 Sigmoid 的优点，同时还引入了概率元素，使得其在处理非线性问题时表现得更为自然。

GELU 函数的数学定义如下：

$$
f(x) = x \cdot \Phi(x)
$$

其中，\(\Phi(x)\) 是标准正态分布的累积分布函数 (CDF)，定义为：

$$
\Phi(x) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x}{\sqrt{2}} \right) \right]
$$

为了简化计算，GELU 通常使用一个近似公式：

$$
f(x) \approx 0.5 \cdot x \cdot \left( 1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left( x + 0.044715 \cdot x^3 \right) \right) \right)
$$

### 2. GELU 的工作机制

GELU 通过引入高斯分布的概率特性，对输入值 \(x\) 的通过概率进行调节，而不是像 ReLU 那样简单地在 \(x > 0\) 时输出 \(x\)，在 \(x \leq 0\) 时输出零。具体来说，GELU 函数将小输入值按一定概率进行抑制，而较大的输入值则按较高的概率通过。

这种行为非常自然：当输入较小时，GELU 函数趋于 0，而当输入较大时，它的输出接近于输入本身。这使得 GELU 函数的非线性更加平滑，并且与数据的自然分布更加契合。

### 3. GELU 与 ReLU、Swish 的比较

- **ReLU**：ReLU 将所有负值直接置零，正值保持不变，这会导致部分神经元在训练时“死亡”（即激活值永远为 0，无法更新）。相比之下，GELU 的输出更加平滑，它根据输入值大小动态调整输出，使网络能更柔和地学习。
  
- **Swish**：Swish 是 Sigmoid 和 ReLU 的结合体，也具有平滑性，并且在很多情况下表现优异。与 Swish 类似，GELU 也在输入为负值时允许负数通过，而不是将它们简单地截断为零。两者的主要区别在于，Swish 是通过 Sigmoid 的方式来进行平滑调整，而 GELU 通过高斯分布函数引入非线性。

### 4. GELU 的优点

- **平滑性**：GELU 函数比 ReLU 和 Leaky ReLU 更加平滑，梯度更新时更加稳定，尤其是在训练复杂的深度神经网络时表现更好。
  
- **概率特性**：GELU 函数根据高斯分布函数动态调整输入的通过概率，这使得它在处理复杂输入时更加灵活，能够更好地适应数据的自然变化。

- **改进性能**：在某些任务（例如自然语言处理）中，GELU 被证明比 ReLU 和 Tanh 等激活函数具有更好的收敛性和性能。这也是为什么像 BERT 这样的模型会采用 GELU。

### 5. GELU 的缺点

- **计算复杂度**：与 ReLU 这样的简单激活函数相比，GELU 的计算更为复杂，尤其是当涉及到正态分布函数时。这可能会在某些任务中引入额外的计算开销。

- **应用场景限制**：尽管 GELU 在某些任务中表现出色，但它未必在所有任务中都能超越其他激活函数。例如，在某些简单的图像分类任务中，ReLU 可能已经足够有效。

### 6. GELU 的实际应用

GELU 被广泛应用于自然语言处理和深度学习模型，特别是在大型 Transformer 模型中，如：

- **BERT**（Bidirectional Encoder Representations from Transformers）
- **GPT 系列模型**
- **ALBERT** 等

这些模型采用 GELU 作为激活函数主要是因为其平滑的非线性和良好的梯度流动，使得模型在复杂的任务中能够更加高效地学习。

### 总结

GELU 是一种优秀的激活函数，它通过结合高斯分布的概率特性，在不同输入范围内动态调整神经元的激活情况。它在一些复杂的任务（特别是自然语言处理）中比传统激活函数表现得更好，能够提高模型的收敛速度和性能。尽管它的计算复杂度较高，但其平滑性和性能优势使其在许多深度学习任务中得到广泛应用。

---

### 激活函数的选择
- **二分类问题**：通常使用 Sigmoid 或 Tanh 作为输出层的激活函数。
- **多分类问题**：Softmax 是最常用的激活函数。
- **隐藏层**：ReLU、Leaky ReLU 和 ELU 是常见的选择，因为它们可以解决梯度消失问题，且训练速度较快。
- **深度神经网络**：Swish 函数在许多现代模型中也表现出色。

激活函数的选择对网络的性能至关重要，不同的激活函数适合不同的任务和网络结构。在选择时，通常会结合经验和实验结果。
