---
layout:     post
title:      时间动作检测技术介绍（Temporal Action Detection）
subtitle:   好痒，要长脑子了
date:       2023-7-7
author:     月月鸟
header-img: img/text-classification.png
catalog: true
tags:
    - Computer Vision
    - Deep Learning
---

时间动作检测（Temporal Action Detection，TAD）是计算机视觉领域中的一个关键任务，主要目标是在视频中检测并定位特定的动作，确定这些动作在时间上的起始和结束时间。与传统的动作分类不同，TAD不仅要求识别出动作的类别，还需要准确定位动作在时间轴上的边界。

# 1. **TAD 的核心任务包括**
   - **动作分类**：给定一个视频片段，识别出视频中执行的动作类型。例如，在一个包含踢足球动作的视频片段中，动作分类的任务是识别出“踢足球”这一动作。
   - **动作定位**：不仅要识别出动作，还要确定动作发生的时间段（即动作的起始和结束时间）以及动作发生的空间区域（即在视频帧中的位置）。


# 2. TAD的主要挑战
- **动作的多样性**：不同的动作有着不同的持续时间、速度和动态表现。这使得在视频中捕捉和区分这些动作变得非常困难。
- **长时间依赖性**：视频中动作的发生通常与前后时刻的背景和其他动作相关，因此需要在较长时间范围内进行依赖关系的建模。
- **动作的时序不均匀性**：同一种动作在不同视频中可能有不同的持续时间，这要求模型具有对不同时间尺度的适应能力。
- **背景干扰**：视频中常常包含大量无关的背景信息或动作，这些干扰会影响模型对目标动作的检测。


# 3. TAD 的常用数据集
有几个常用的数据集，这些数据集为研究和评估提供了基础。以下是一些主要的TAD数据集：
## 3.1. **THUMOS**
- **版本**：THUMOS14
- **简介**：THUMOS14是一个广泛使用的数据集，专注于体育动作检测。它包括两个部分：THUMOS13用于动作分类，THUMOS14用于动作检测。THUMOS14数据集包括20个动作类别，提供了大量的短视频片段，每个片段都标注了动作的开始和结束时间。
- **数据量**：包含413个验证视频和213个测试视频，总共16,354个动作实例。
- **用途**：主要用于研究动作检测任务，特别是在体育视频中的应用。

## 3.2. **ActivityNet**
- **版本**：ActivityNet v1.3
- **简介**：ActivityNet是一个大型数据集，涵盖了200种不同的日常活动。与THUMOS相比，ActivityNet包含的动作类型更加多样，视频时长也更长，动作更加复杂和自然。数据集的视频都是从YouTube上收集的，且大多是非专业拍摄的日常活动。
- **数据量**：包含19,994个视频，分为训练集、验证集和测试集。
- **用途**：广泛用于研究时间动作检测、动作分类和动作定位等任务。

## 3.3. **HACS (Human Action Clips and Segments)**
- **简介**：HACS是一个大规模数据集，专注于动作识别和检测任务。它包含了从YouTube上收集的动作片段，涵盖了200个动作类别。HACS的数据量较大，视频的质量和多样性也比较高。
- **数据量**：包含50万个动作片段和1.4万个完整的视频。
- **用途**：用于研究大规模时间动作检测任务。

## 3.4. **Charades**
- **简介**：Charades是一个以日常家庭活动为主的数据集，专门设计用于动作识别和时间动作检测。它包含的动作片段相对较长，动作类型多样且具有挑战性。每个视频包含多个动作片段，并且不同动作之间可能会有重叠。
- **数据量**：包含9,848个视频，约67,000个动作实例。
- **用途**：用于复杂场景中的时间动作检测，特别是处理多个重叠动作的情况。

## 3.5. **AVA (Atomic Visual Actions)**
- **简介**：AVA数据集专注于细粒度的动作检测和时空动作定位。它通过在每秒的帧上进行动作标注，提供了视频中每个时间点的动作类别。AVA的动作类别涵盖了较为具体和精细的动作类型，如打招呼、行走、跑步等。
- **数据量**：包含约430个15分钟的电影剪辑，总共57万个动作实例。
- **用途**：适用于研究细粒度时间动作检测和动作的时空定位。

## 3.6. **EPIC-KITCHENS**
- **简介**：EPIC-KITCHENS数据集专注于厨房环境下的活动和手部动作检测。视频记录了第一人称视角的厨房活动，包括大量的日常任务，如切菜、倒水等。
- **数据量**：包含约55小时的高清视频，共有约39,600个动作实例。
- **用途**：用于第一人称视角下的时间动作检测和手部动作识别。

## 3.7. **UCF101-24**
- **简介**：UCF101是一个常用的动作识别数据集，其中的UCF101-24子集被用于时间动作检测任务。该子集包含了24种动作类别，主要是体育活动和一些日常动作。
- **数据量**：包含3,207个视频片段。
- **用途**：用于动作分类和时间动作检测任务，特别是在体育视频中的应用。

## 3.8. **Kinetics-GEBD (Generalized Event Boundary Detection)**
- **简介**：Kinetics-GEBD扩展了Kinetics数据集，专注于事件边界检测。它标注了视频中每个事件的开始和结束时间，覆盖了丰富的动作类型。
- **数据量**：包含约30万个视频片段，涵盖不同的动作和事件类型。
- **用途**：用于研究事件边界检测和时间动作检测任务。


# 4. TAD的评价指标
1. **mAP（mean Average Precision）**：mAP是TAD领域最常用的综合性指标。它计算了模型在所有动作类别上的平均精度，通常在不同的时间IoU（t-IoU）阈值下计算，例如0.5、0.75和0.95。mAP可以评估模型在不同定位精度要求下的总体性能。

2. **时间IoU（Temporal Intersection over Union, t-IoU）**：t-IoU用于衡量预测的动作时间区间与真实时间区间的重合度。在TAD评估中，常使用不同的t-IoU阈值（如0.5、0.75）来判断预测是否准确。例如，当t-IoU≥0.5时，预测才被认为是正确的。

3. **Precision（精度）**：精度指的是模型检测出的动作中有多少是正确的。它衡量了模型的误报率，精度越高，说明模型在检测时的误报越少。

4. **Recall（召回率）**：召回率指的是所有实际存在的动作中有多少被模型成功检测到。高召回率意味着模型能够识别出大多数实际发生的动作，但可能会有较多的误报。

5. **F1 Score**：F1 Score是精度和召回率的调和平均值，用于平衡这两个指标。当需要在精度和召回率之间取得平衡时，F1 Score是一个有用的指标。

# 5. TAD的应用
时间动作检测（Temporal Action Detection，TAD）技术在多个领域有广泛的应用，以下是一些主要的应用场景：

## 5.1. **视频监控**
- **应用场景**：在公共安全和安防领域，TAD技术可以用来实时检测和识别视频监控中的异常行为，如打斗、抢劫或逃跑等。
- **优势**：通过TAD技术，监控系统可以自动识别并标记可疑动作，减少人工监控的负担，提高安全响应的速度和准确性。

## 5.2. **体育分析**
- **应用场景**：在体育比赛中，TAD可以用于自动分析比赛视频，识别并标注关键动作，如进球、犯规、扣篮等。
- **优势**：这不仅能帮助教练和运动员进行比赛回顾和技术分析，还能为观众提供高质量的比赛回放和精彩片段集锦。

## 5.3. **人机交互**
- **应用场景**：TAD可以用于增强虚拟现实（VR）和增强现实（AR）中的人机交互，通过检测用户的动作来触发相应的虚拟反应，如手势控制、体感游戏等。
- **优势**：这种应用能够提供更自然和直观的交互方式，提升用户体验，特别是在游戏、教育和远程协作等领域。

## 5.4. **视频内容检索**
- **应用场景**：在大规模视频数据库中，TAD可以用于快速定位和检索包含特定动作的视频片段，如在影视片段中查找打斗场面或舞蹈动作。
- **优势**：这种技术可以极大地提高视频检索的效率，尤其在需要快速定位某些关键内容的场景中，如新闻报道、影视制作和视频广告等。

## 5.5. **智能家居**
- **应用场景**：在智能家居系统中，TAD技术可以用于监控和分析家庭成员的日常活动，如跌倒检测、运动量分析等。
- **优势**：这有助于及时发现异常情况并采取相应的措施，特别是对于老年人和需要特殊照顾的人群，能够提供更好的安全保障。

## 5.6. **医疗健康**
- **应用场景**：TAD可以用于康复训练监控和运动医学，帮助医生和物理治疗师监控病人的康复进展，如步态分析、运动幅度监控等。
- **优势**：通过准确监测和分析病人的动作，能够提供更个性化的治疗方案，促进患者的快速恢复。

## 5.7. **无人驾驶和智能交通**
- **应用场景**：在自动驾驶和智能交通系统中，TAD技术可以用于检测和预测道路上行人的动作，如突然横穿马路或跑步。
- **优势**：这种应用可以提高车辆的安全性，通过提前检测和应对潜在危险，减少交通事故的发生。

## 5.8. **教育与培训**
- **应用场景**：在教育和培训中，TAD可以用于自动分析和反馈学员的动作，如体操训练、舞蹈课程等。
- **优势**：这能够帮助学员更快地掌握技能，并且提供实时的纠错和改进建议。

## 5.9. **影视制作**
- **应用场景**：在影视制作中，TAD可以用于剪辑和标注电影或视频中的特定动作场景，帮助编辑快速找到和处理需要的片段。
- **优势**：这种技术能够大大提高编辑效率，并且在制作精彩片段集锦或预告片时提供技术支持。


# 6. 我的项目使用的TAD方法

## 6.1 Actionformer模型

### 6.1.1 简介
**研究背景：** 视频中的动作时间定位（Temporal Action Localization, TAL）是指在视频中准确确定动作发生的时刻并识别其类别。这一技术在视频理解、事件检测等领域具有广泛的应用前景。

**挑战：** 传统的TAL方法通常依赖于动作提议（action proposals）或锚框（anchor boxes），并利用复杂的网络结构，如卷积神经网络、循环神经网络或图神经网络，来捕捉视频中的长时上下文信息。然而，这些方法往往计算复杂度高，依赖于提议生成和锚框设计，导致模型训练和推理过程中的复杂性及计算负担较大。

**研究目标：** 本文提出了一种基于Transformer的模型——**ActionFormer**。该模型通过引入局部自注意力机制和多尺度特征表示，旨在实现对视频中动作的高效时间定位，避免使用传统的动作提议或锚框，从而简化模型训练过程并降低计算复杂度。ActionFormer不依赖于动作建议或锚窗口，而是直接对视频中的每一刻进行分类并回归其动作边界，形成一个简单高效的单阶段（single-stage）模型。

### 6.1.2 ActionFormer模型结构
ActionFormer模型是一种用于视频时间动作定位（Temporal Action Localization, TAL）的创新模型。它通过Transformer模型来识别视频中的动作实例，并确定这些动作发生的时间边界。ActionFormer的设计简洁而高效，结合了局部自注意力机制和多尺度特征金字塔，解决了传统方法中的一些难题。下面是对ActionFormer模型的详细讲解：

#### 6.1.2.1. **模型概述**
ActionFormer采用了一个**编码器-解码器**的架构，整体设计思想是在不使用动作提议或预定义锚框的情况下，直接对视频中的每个时间点进行分类和边界回归，从而实现单阶段的动作定位。

#### 6.1.2.2. **输入表示**
模型的输入是一个视频特征序列。假设一个输入视频被分为 \( T \) 个时间步（time steps），每个时间步对应一个特征向量 \( x_t \)，这些特征向量通常由预训练的3D卷积神经网络（如I3D网络）提取。整个输入序列表示为 \( X = \{x_1, x_2, ..., x_T\} \)。

#### 6.1.2.3. **特征投影**
在进入Transformer编码器之前，输入的特征序列首先通过一个浅层的卷积网络进行投影。这一步骤主要是为了：
- 将输入特征映射到更高维的空间，从而增强模型的表达能力。
- 稳定训练过程，并为后续的Transformer编码提供更加适合的特征表示。

特征投影由两层1D卷积层组成，每层之后都附有ReLU激活函数。

#### 6.1.2.4. **Transformer编码器**
Transformer编码器是ActionFormer的核心部分。它通过多个Transformer层（Layers）对输入特征进行编码，生成多尺度的特征金字塔。编码器的关键设计点包括：

##### a. **局部自注意力机制**
Transformer的自注意力机制本质上是对整个输入序列进行全局的特征关系建模。然而，在视频时间动作定位中，长时间依赖关系虽然重要，但全局自注意力的计算复杂度较高。为了解决这个问题，ActionFormer采用了局部自注意力机制（Local Self-Attention）：
- **局部窗口**：每个时间点只与它的局部时间范围内的特征进行交互，而非与整个输入序列。这显著降低了计算复杂度，并且有助于模型聚焦于动作发生的局部上下文。
- **计算复杂度**：通过限制注意力机制在局部窗口内进行计算，复杂度从 \(O(T^2D)\) 降低到 \(O(W^2TD)\)，其中 \(W\) 是局部窗口的大小。

##### b. **多尺度特征金字塔**
多尺度特征金字塔的设计灵感来自于计算机视觉中物体检测的特征金字塔网络（FPN）。在ActionFormer中，Transformer编码器通过多层次的Transformer单元对输入特征进行处理，每一层输出的特征表示不同时间尺度上的信息：
- **多层次设计**：编码器包含多个Transformer层，每层都包括局部自注意力单元和多层感知机（MLP）单元。
- **下采样操作**：为了生成多尺度特征金字塔，每个Transformer层之后可以进行下采样操作，这通常通过1D深度卷积完成。
- **特征表示**：最终，编码器输出一组不同时间尺度的特征表示 \( Z = \{Z_1, Z_2, ..., Z_L\} \)，每个特征层代表不同的时间分辨率。

#### 6.1.2.5. **解码器**
解码器负责从编码器输出的多尺度特征中提取出动作类别和边界信息。它由两个主要部分组成：分类头和回归头。

##### a. **分类头**
分类头用于确定每个时间点是否属于某个动作类别，以及属于哪个类别。具体实现上，分类头是一个轻量级的1D卷积网络，包含多层卷积和ReLU激活：
- **多层卷积**：分类头使用了三层1D卷积层，层间带有层归一化（Layer Normalization）和ReLU激活。
- **输出概率**：最终输出为每个时间点属于不同动作类别的概率，通过sigmoid激活函数实现。

##### b. **回归头**
回归头用于预测每个时间点距离动作的开始和结束时间的距离（即边界信息）：
- **回归范围**：模型为每个时间点输出两个回归值，分别表示到动作起始点和结束点的距离。如果该时间点属于动作类别，则这两个距离值有效。
- **卷积网络**：回归头的结构与分类头类似，也是由1D卷积网络实现，但输出激活函数使用ReLU，以确保回归值为正。

#### 6.1.2.6. **损失函数**
ActionFormer的总损失由分类损失和回归损失两部分组成：

##### a. **分类损失**
分类损失使用了Focal Loss，用于解决正负样本不平衡的问题。在动作定位任务中，大多数时间点属于背景（非动作），因此Focal Loss可以减少背景样本对训练的不利影响。

##### b. **回归损失**
回归损失使用了DIoU（Differentiable IoU）损失，这是一种优化后的IoU损失，能够更好地优化动作边界的预测。

##### c. **中心采样**
中心采样策略用于在训练过程中更关注动作中心附近的时间点。只有距离动作中心较近的时间点才被视为正样本，这有助于模型在动作的中心区域得到更高的分类准确率。

#### 6.1.2.7. **模型推理**
在推理阶段，ActionFormer通过对视频特征进行全序列处理来预测每个时间点的动作类别和边界：
- **时间步预测**：模型为每个时间步输出分类概率和边界回归值，分类头决定是否存在动作以及动作类别，回归头则确定该动作的时间边界。
- **后处理**：通过Soft-NMS（Non-Maximum Suppression）对预测结果进行后处理，以去除重叠较大的动作实例，得到最终的动作定位结果。

#### 6.1.2.8. **实验与结果**
ActionFormer通过一系列实验在多个数据集上验证了其优越性：
- **性能优越**：在THUMOS14、ActivityNet 1.3和EPIC-Kitchens 100等多个数据集上，ActionFormer在多个tIoU阈值下的mAP表现显著优于现有方法。
- **消融实验**：论文还通过消融实验验证了各个设计选择（如局部自注意力窗口大小、多尺度特征层数）的有效性。

#### 6.1.2.9. **模型优势与局限**
ActionFormer模型的优势在于其简洁的设计，避免了复杂的动作提议生成过程，同时通过多尺度特征金字塔和局部自注意力机制提升了时间动作定位的精度和效率。然而，其依赖于预训练的视频特征，并且在处理复杂背景或包含大量动作实例时可能存在一定的误差。

#### 6.1.2.10. **未来工作**
未来的工作可能包括：
- **改进Transformer架构**：进一步优化Transformer的设计，使其在长视频序列中表现更好。
- **无监督学习**：探索无监督或半监督学习方法，以减少对人工标注的依赖，并提升模型的泛化能力。


## 6.2 TriDet模型

### 6.2.1 简介

这篇论文《TriDet: Temporal Action Detection with Relative Boundary Modeling》提出了一种新的一阶段框架TriDet，用于时序动作检测（TAD）。现有的方法通常在视频中由于动作边界模糊而导致边界预测不精确。为了解决这个问题，作者提出了一个新颖的Trident-head，通过估计边界周围的相对概率分布来对动作边界进行建模。

### 6.2.2 TriDet模型结构

TriDet模型的结构是一个创新的时序动作检测（TAD）框架，其主要特点是通过一种称为Trident-head的检测头来对动作边界进行相对概率建模，并结合了一种高效的可扩展粒度感知（Scalable-Granularity Perception，SGP）层来处理视频特征。以下是对模型结构的详细解析：

#### 6.2.2.1. 模型总体结构

TriDet由三个主要部分组成：
- **视频特征提取骨干网络（Backbone Network）**：用于从输入的视频中提取时间序列特征。
- **SGP特征金字塔（Feature Pyramid with SGP Layer）**：用于在不同的时间尺度上对特征进行处理和聚合。
- **Trident-head检测头（Trident-head Detection Head）**：用于动作边界的精确定位。

#### 6.2.2.2. 视频特征提取骨干网络
TriDet首先使用一个预训练的动作分类网络（如I3D或SlowFast）来提取视频特征。这些网络可以捕捉视频中时间和空间的信息，从而生成一个时间序列特征表示，这些特征随后被传递给SGP特征金字塔进行进一步处理。

#### 6.2.2.3. SGP特征金字塔
SGP特征金字塔是TriDet的核心组件之一，它负责在不同的时间尺度上处理视频特征。该金字塔通过以下几个步骤构建：

- **特征下采样**：从视频特征提取骨干网络输出的特征开始，特征在金字塔的每个层级上通过最大池化（stride为2）进行下采样，逐步降低时间分辨率。
  
- **SGP层（Scalable-Granularity Perception Layer）**：
  - **Instant-level分支**：在每个时间点上，通过增加动作与非动作特征之间的距离来提高特征的可区分性。具体来说，它通过视频级别的平均特征来调整每个时间点的特征。
  - **Window-level分支**：在更大感受野的范围内引入语义内容，通过一个可扩展因子k来动态调整不同尺度的时间信息，从而捕捉动作在不同时间尺度上的特征。
  
SGP层的设计旨在解决自注意力机制的两个主要问题：
- **秩损失问题**：在时间维度上的自注意力机制可能导致特征之间的相似性增加，从而降低特征的可区分性。
- **计算复杂度高**：自注意力机制的密集计算带来较高的计算开销，而SGP层通过卷积操作大幅降低了这种开销。

#### 6.2.2.4. Trident-head检测头
Trident-head是TriDet用来精确定位动作边界的检测头。它由三个主要部分组成：
- **开始边界头（Start Boundary Head）**：预测动作的开始边界。
- **结束边界头（End Boundary Head）**：预测动作的结束边界。
- **中心偏移头（Center Offset Head）**：估计动作中心相对于时间点的偏移量。

##### 6.2.2.4.1 相对边界建模
Trident-head的创新在于，它通过估计某个时间段内每个时间点作为边界的相对概率来建模边界，而不是直接回归边界偏移量。具体过程如下：
- **边界响应**：对于每个时间点，Trident-head分别预测它作为动作开始或结束的可能性（边界响应）。
- **中心偏移**：然后根据该时间点的中心偏移头输出，预测该时间点在邻近范围内作为边界的相对概率分布。
- **边界距离计算**：最后，使用相对概率分布计算边界的偏移距离。

这种建模方式比传统的单点回归方法更为稳定，因为它考虑了相邻时间点之间的关系，从而在动作边界模糊的情况下仍然能够做出精确的预测。

#### 6.2.2.5. 训练和推理
- **训练**：TriDet的训练使用了一个端到端的损失函数，该损失函数结合了分类损失和回归损失，并采用了中心采样策略来确定正样本和负样本。
- **推理**：在推理阶段，TriDet会筛选出分类得分高于某个阈值的时间点，然后对这些时间点进行动作实例的检测，并应用Soft-NMS（非极大值抑制）来去除冗余的预测结果。

#### 6.2.2.6. 计算效率
TriDet通过SGP层取代了传统的自注意力机制，显著降低了计算成本，并在推理速度上有明显提升，同时在多个基准数据集上实现了最先进的性能。通过这种结构，TriDet能够在保持较高检测精度的同时，大幅减少计算资源的消耗，使其成为一个高效且精准的时序动作检测模型。

## 6.3. 两个模型的对比

TriDet和ActionFormer模型都是用于时序动作检测（Temporal Action Detection, TAD）的先进模型，它们在任务上有许多相似之处，但也有关键的区别。以下是它们的联系和区别的详细解析：

### 6.3.1. 联系

1. **任务相同**：两者都旨在解决TAD任务，即在未裁剪的视频中检测并定位所有动作的开始和结束时间以及相应的动作类别。

2. **Transformer的使用**：ActionFormer是一个基于Transformer的模型，它利用自注意力机制来捕捉时间特征。虽然TriDet最终采用了不同的机制（SGP层），但其发展和设计明显受到Transformer技术的启发，特别是在特征处理和时间特征建模方面。

3. **特征金字塔**：两者都使用了特征金字塔结构来处理不同时间尺度上的特征。特征金字塔允许模型在不同时间分辨率下捕捉动作，从而提高了对长短不同的动作的检测能力。

4. **目标**：两个模型都旨在提高边界检测的准确性，这是TAD任务中的一个关键挑战。两者都认识到动作边界往往是模糊的，并尝试通过不同的方法来提升边界的预测精度。

### 6.3.2. 区别

1. **基础结构的不同**：
   - **ActionFormer**：主要依赖于Transformer架构，使用自注意力机制（Self-Attention, SA）来捕捉视频特征的时间依赖性。这种机制能够非常有效地建模长距离的时间依赖，但也会带来计算复杂度的增加，并且在某些情况下可能导致特征的秩损失问题。
   - **TriDet**：TriDet提出了一种基于卷积网络的可扩展粒度感知层（SGP层），取代了Transformer中的自注意力机制。SGP层通过卷积操作来捕捉时间特征，并通过多尺度处理提高特征的可区分性，同时大幅降低计算成本。

2. **边界建模**：
   - **ActionFormer**：采用了一种基于局部窗口的自注意力方法来定位动作边界，这种方法利用了局部特征，但仍然存在边界模糊的挑战。
   - **TriDet**：引入了Trident-head，这是一种通过相对概率建模来检测边界的机制。这种方法不仅考虑了单个时间点的特征，还考虑了相邻时间点之间的关系，从而在预测边界时更加精确和稳健。

3. **计算效率**：
   - **ActionFormer**：由于使用了自注意力机制，其计算复杂度较高，尤其是在处理长视频序列时会增加推理时间。
   - **TriDet**：通过SGP层和Trident-head的结合，TriDet在保持高检测精度的同时显著降低了计算复杂度，并且在推理速度上更具优势。

### 6.3.3. 总结

TriDet和ActionFormer在时序动作检测任务上有许多共同点，特别是在利用特征金字塔结构和追求高边界检测精度方面。然而，TriDet通过引入SGP层和Trident-head，在降低计算复杂度和提高边界检测精度方面进行了显著创新。可以说，TriDet是对ActionFormer的一种优化和扩展，旨在更高效地处理TAD任务中的挑战。


