---
layout:     post
title:      大模型笔记（LLM）
subtitle:   大语言模型基础3
date:       2024-9-6
author:     月月鸟
header-img: img/LLM.jpg
catalog: true
tags:
    - LLM
---

# 1. 词嵌入
##  1.1. Word2Vec 概述

Word2Vec 是 Google 在 2013 年推出的一款自然语言处理工具。它的核心功能是将单词转化为向量，使得词与词之间可以通过向量间的距离进行定量测量，从而挖掘词与词之间的关联性。

实际上，用向量表示词的想法早在 Word2Vec 之前就已经存在。最早的词向量表示采用的是 One-Hot 编码，每个词向量的维度与词汇表的大小相同。对于词汇表中的每个词，其对应的向量只有一个位置为 1，其他位置都为 0。例如，如果有 5 个词组成的词汇表：

One-Hot 编码方式虽然简单，但它有两个显著的缺点：
1. 词汇表通常很大，可能包含上百万个词，因此高维向量的处理会消耗大量计算资源和时间。
2. One-Hot 编码生成的向量彼此正交，未能体现词与词之间的相似性和关联性。

针对这些问题，分布式表示（Distributed Representation）应运而生。它通过训练将 One-Hot 编码的高维向量映射为较低维度的向量，这个维度可以根据任务需求灵活设置。

例如，在下图中，词汇表中的词被映射到 "Royalty"、"Masculinity"、"Femininity" 和 "Age" 四个维度上，“King” 这个词可能对应向量 (0.99, 0.99, 0.05, 0.7)。虽然实际情况中并不能对每个维度进行明确解释，但分布式表示使得词向量间保留了一定的语义关系。

通过这样的词向量，我们可以更直观地分析词之间的关系。例如，将词向量降维至二维后，有研究发现，不同词之间的相对位置可以揭示它们的关联性。

那么，如何训练才能得到这些合适的词向量呢？为了解决这个问题，Google 的 Tomas Mikolov 在论文中提出了两种神经网络模型：CBOW 和 Skip-gram。

## 1.2. Word2Vec 原理

Word2Vec 的训练模型本质上是一个仅有一个隐含层的神经网络，如下图所示：

- 输入层为采用 One-Hot 编码的词向量，输出层也是 One-Hot 编码。
- 经过训练后，输入层到隐含层的权重矩阵就是每个词的分布式表示（词向量）。
  
例如，上图中的矩阵 \( W_{V \times N} \) 的第 i 行的转置，就是词汇表中第 i 个词的低维向量表示。这样，原始维度为 \( V \) 的词向量被映射为维度为 \( N \) 的词向量（其中 \( N \) 远小于 \( V \)），并且词向量之间保留了某种关联关系。

Mikolov 在论文中提出的 CBOW 和 Skip-gram 是两个不同的模型：
- **CBOW** 适用于较小的数据集。
- **Skip-gram** 则在大型语料库中表现更好。

CBOW 使用目标词的上下文单词作为输入，输出目标词；而 Skip-gram 则相反，它以目标词为输入，预测该词周围的上下文。

##  1.3. CBOW 模型

CBOW 模型的工作过程如下：

1. **输入层**：上下文单词的 One-Hot 编码词向量。假设输入为（is, an, on, the），每个单词的向量维度为词汇表的大小 \( V \)，上下文单词数量为 \( C \)。
2. **隐含层**：初始化一个权重矩阵 \( W_{V \times N} \)，用输入的词向量与该矩阵相乘，得到维度为 \( N \) 的向量 \( \omega_1, \omega_2, \ldots, \omega_C \)。这些向量的平均值作为隐藏层向量 \( h \)。
3. **输出层**：初始化另一个权重矩阵 \( W'_{N \times V} \)，将隐藏层向量 \( h \) 与该矩阵相乘，得到一个 \( V \) 维的向量 \( y \)。这个向量经过激活函数处理，得到词汇表中各个词的概率分布。
4. **误差计算与优化**：计算预测结果与实际目标词的 One-Hot 编码之间的误差（通常使用交叉熵损失函数），并使用梯度下降法更新两个权重矩阵。

经过训练后，每个词的向量就可以直接从权重矩阵 \( W \) 中提取出来，成为分布式表示的词向量，即 Word Embedding。

## 1.4. Skip-gram 模型

Skip-gram 模型与 CBOW 的主要区别在于它是通过给定的中心词来预测其上下文。模型结构如下图所示：

Skip-gram 的工作流程为：
1. 选择一个句子中间的词作为输入词（input word），如选取 "apple"。
2. 定义一个参数 skip_window，表示从输入词的左侧和右侧分别选择多少个词作为上下文。例如，skip_window=2 时，输入词为 “apple”，上下文为 ["is", "an", "on", "the"]。
3. 生成 (input word, output word) 形式的训练样本，例如 ('apple', 'an')，('apple', 'on')。
4. 神经网络基于这些样本计算输入词和上下文词之间的共现概率分布。
5. 通过梯度下降和反向传播更新模型权重，最终得到每个词的低维向量表示。

## 1.5. 模型优化：Hierarchical Softmax

Hierarchical Softmax 对原有模型进行了优化：
1. 从输入层到隐藏层的映射简化为输入词向量的求和。
2. 使用哈夫曼树代替原来的输出层矩阵 \( W' \)。哈夫曼树的叶节点代表词汇表中的词，每个路径对应于一个词的词向量。

这种方法显著减少了计算量，特别是对于高频词的处理效率更高。

## 1.6. 模型优化：Negative Sampling

尽管 Hierarchical Softmax 提高了模型效率，但对于低频词的计算仍然有较大开销。Negative Sampling 提供了另一种有效的方法。

在负采样中，对于每个正样本（input word, output word），仅更新少量的负样本（随机选取的不相关词汇），而不是更新整个词汇表的概率分布。这样可以大幅降低训练过程中的计算复杂度。

总结来说，Word2Vec 通过 CBOW 和 Skip-gram 模型将词转化为低维向量，并通过 Hierarchical Softmax 和 Negative Sampling 进一步提升训练效率，使得词向量能更有效地捕捉词与词之间的关系，从而在众多 NLP 任务中发挥出色的表现。

---

# 自然语言处理三大特征提取器：CNN、RNN 与 Transformer

在自然语言处理（NLP）领域，CNN、RNN 和 Transformer 是三种主要的特征提取器。随着技术的发展，它们各自的定位也逐渐清晰：

- **RNN** 已经基本完成了其历史使命，逐步退出舞台；
- **CNN** 如果经过合适的改造，仍然有机会在 NLP 领域占据一席之地；
- **Transformer** 则迅速成为 NLP 任务中最主流的特征提取器，未来大有可为。

### NLP 任务的特点

NLP 任务有几个显著的特点：
1. 输入是一个一维线性序列，且长度不固定；
2. 单词或句子之间的位置关系至关重要；
3. 长距离依赖的语义关系对理解句子的完整含义非常关键。

特征提取器是否能够充分适应这些特点，往往决定了其在 NLP 领域的表现。因此，许多模型的改进方向都是为了更好地适应这些任务特性。

### RNN 的特性与局限

**RNN（循环神经网络）** 以线性序列的方式从前向后逐步收集输入信息，非常适合处理 NLP 的线性序列输入。由于其能够处理不定长输入，并且可以在信息传播过程中保持上下文关系，因此曾经在 NLP 领域非常流行。为了缓解反向传播中常见的梯度消失和梯度爆炸问题，引入了 LSTM（长短期记忆）和 GRU（门控循环单元）模型，通过增加中间状态来直接向后传递信息，使得模型可以捕获较长距离的依赖关系。

LSTM 和 GRU 的引入显著增强了 RNN 的能力，随后 NLP 领域还借鉴了图像处理领域的注意力机制（Attention），并结合了多层网络和 Encoder-Decoder 框架。这些改进极大地提升了 RNN 的性能和应用范围。

尽管如此，RNN 依然面临两大挑战：
1. **新模型的崛起**：如经过改造的 CNN 和 Transformer。
2. **并行计算能力弱**：RNN 的序列依赖性使其在大规模并行计算中效率较低。

### CNN 的特性与改进

**CNN（卷积神经网络）** 在 NLP 中的应用主要是通过捕获 k-gram 特征，即单词的局部片段信息。k 的大小决定了模型能捕获多远距离的特征关系。NLP 领域中常用的 CNN 通常使用 1-D 卷积层叠加深度，并配合 Skip Connection（跳跃连接）或 Dilated CNN（膨胀卷积）等技术进行优化。

CNN 的卷积层能够保留相对位置信息，同时具有极强的并行计算能力，使其在 NLP 中具有一定优势。然而，原生 CNN 在捕捉长距离依赖关系上不如 RNN 和 Transformer。

### Transformer 的特点

**Transformer** 的崛起改变了 NLP 领域的格局。它解决了传统 RNN 和 CNN 模型在处理不定长序列时的挑战。Transformer 的做法是设定输入的最大长度，不足时通过填充（Padding）来使输入达到统一长度，从而适应并行计算。

Transformer 的优势在于其强大的语义特征提取能力和对长距离依赖关系的捕捉能力。相比 RNN 和 CNN，Transformer 可以更高效地建模全局关系，尤其是在处理长句子时表现出色。

### 三大特征提取器的对比

1. **语义特征提取能力**：Transformer 显著优于 RNN 和 CNN，后两者在这方面差距不大。
2. **长距离特征捕获能力**：原生 CNN 在捕获长距离依赖关系上表现较弱，而 Transformer 略胜于 RNN。但在主语与谓语间距超过 13 个词时，RNN 反而表现更好，因此整体来看，Transformer 与 RNN 在这方面能力相近，而 CNN 则明显落后。
3. **任务综合特征提取能力**：在机器翻译等复杂任务中，Transformer 的表现明显优于 RNN 和 CNN，而 RNN 和 CNN 表现相当，CNN 略占上风。
4. **并行计算能力与效率**：RNN 因其序列依赖性，在并行计算中效率较低；而 CNN 和 Transformer 由于无此限制，可以更轻松地进行并行优化，表现远超 RNN。

### 综合评价与未来趋势

从综合表现来看，Transformer 明显优于 CNN 和 RNN，特别是在处理复杂任务和并行计算方面具有显著优势。虽然 CNN 在某些场景下略优于 RNN，但与 Transformer 相比仍有差距。

**未来趋势**：随着 Transformer 的不断发展，越来越多的模型正向其靠拢，CNN 和 RNN 也在通过融合 Transformer 的思想进行改造，以期适应 NLP 领域的新需求。最终，Transformer 将在 NLP 任务中继续担任主角，而 CNN 和 RNN 则更多地在特定任务中找到自己的定位。

# 3. 激活函数

### 激活函数概述

激活函数在神经网络中引入了非线性能力，解决了线性模型无法处理非线性问题的局限性。不同的激活函数具有各自的特点和适用场景。

1. **Sigmoid 和 Tanh**：将输出限制在 (0,1) 和 (-1,1) 之间，因此适合概率值处理，例如 LSTM 中的门机制。由于输出范围的限制，它们容易导致梯度消失，不适合深层网络。
2. **ReLU（Rectified Linear Unit）**：适用于深层网络的训练，不存在 Sigmoid 和 Tanh 的梯度消失问题，但可能出现梯度爆炸。

### 梯度爆炸与梯度消失

神经网络训练过程中，梯度爆炸和梯度消失是常见问题：

- **梯度消失**：通常是由 Sigmoid 和 Tanh 激活函数引起的。由于其导数值在 (0,1) 范围内，反向传播时梯度会逐步减小，最终导致参数无法有效更新。此外，矩阵连乘也会导致梯度消失，当所有梯度值小于 1 时，连乘结果会迅速趋近于零。
- **梯度爆炸**：当梯度值大于 1 时，经过多次连乘后会迅速增大，导致参数值不稳定。

**解决方法**：
- 使用 ReLU 等不会导致梯度消失的激活函数。
- 对梯度进行截断、使用残差连接和正则化（如 Batch Normalization）来缓解梯度爆炸和消失问题。

### 常见激活函数

1. **Sigmoid**

   - 公式：\( \sigma(z) = \frac{1}{1 + e^{-z}} \)
   - 导数：\( \sigma'(z) = \sigma(z)(1 - \sigma(z)) \)
   - **优点**：输出平滑，适用于二分类问题和概率输出。
   - **缺点**：计算复杂、容易出现梯度消失，输出均值非零，可能影响深层网络的训练效果。

2. **Tanh**

   - 公式：\( \text{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \)
   - 导数：\( \text{tanh}'(x) = 1 - (\text{tanh}(x))^2 \)
   - **优点**：输出范围为 (-1, 1)，解决了 Sigmoid 输出非零均值的问题。
   - **缺点**：计算复杂，梯度消失问题依然存在，但比 Sigmoid 要轻。

3. **ReLU（Rectified Linear Unit）**

   - 公式：\( \text{ReLU}(z) = \max(0, z) \)
   - **优点**：计算简单，避免梯度消失问题，适合深层网络训练。
   - **缺点**：当 \( z < 0 \) 时，导数为 0，可能导致神经元“死亡”，即 Dead ReLU 问题。容易导致梯度爆炸，且输出均值非零。

4. **Leaky ReLU**

   - 为了解决 ReLU 的 Dead ReLU 问题，Leaky ReLU 在 \( z < 0 \) 时引入了一个小的斜率（如 0.01），使得负值也能进行微小更新。
   - **优点**：一定程度上缓解了神经元死亡问题。
   - **缺点**：效果不稳定，实际使用较少。

5. **PReLU 和 RReLU**

   - **PReLU**：Parametric ReLU，负值区域的斜率是可学习的参数。
   - **RReLU**：Random ReLU，负值区域的斜率在训练时随机生成。
   - **优点**：灵活性高，对不同数据适应性更好。
   - **缺点**：训练时不易调优。

6. **ELU（Exponential Linear Unit）**

   - 公式：\( \text{ELU}(x) = \begin{cases} 
      x, & \text{if } x > 0 \\
      \alpha (e^x - 1), & \text{if } x \leq 0 
   \end{cases} \)
   - **优点**：负值区域具有负输出，均值接近于 0，使得模型学习更快，缓解梯度消失问题。
   - **缺点**：计算量稍大。

7. **GELU（Gaussian Error Linear Unit）**

   - 出自 2016 年论文《Gaussian Error Linear Units (GELUs)》。
   - 公式：\( \text{GELU}(x) = x \cdot \Phi(x) \)，其中 \( \Phi(x) \) 是标准正态分布的累积分布函数。
   - **优点**：在负值区域不再全为 0，解决了 Dead ReLU 问题，函数光滑可导。
   - **近似公式**：\( \text{GELU}(x) \approx 0.5 \cdot x \cdot (1 + \tanh(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3))) \)，适用于加速计算。

8. **Swish**

   - 出自 2017 年论文《Searching for Activation Functions》。
   - 公式：\( \text{Swish}(x) = x \cdot \sigma(x) \)
   - **优点**：无上限，不存在梯度饱和问题，且在负值区域有渐进性输出，可以更好地保留信息。
   - **缺点**：相对于 ReLU，计算量略大。

9. **GLU（Gated Linear Unit）**

   - 出自 2017 年论文《Language Modeling with Gated Convolutional Networks》。
   - 公式：\( \text{GLU}(x) = x \otimes \sigma(g(x)) \)，其中 \( g(x) \) 通常是一个经过卷积或 MLP 的映射。
   - **优点**：通过门机制控制信息的流动，使模型能够更灵活地筛选重要特征。

### 总结

激活函数是神经网络的核心部分，不同的激活函数适用于不同的场景和需求：
- 对于深层网络，**ReLU** 及其变种（如 Leaky ReLU、PReLU、ELU）更适合。
- 对于需要更平滑且连续性的场景，**GELU** 和 **Swish** 提供了更精细的特性。
- 在需要处理概率值的任务中，**Sigmoid** 和 **Tanh** 依然有其独特的应用场景。

在现代深度学习中，**Transformer** 等架构的流行进一步推动了 GELU 和 Swish 这样的激活函数的使用，它们的柔性和可控性使得模型能够更好地捕捉复杂的非线性关系。

