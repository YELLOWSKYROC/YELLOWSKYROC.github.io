---
layout:     post
title:      无监督学习-降维
subtitle:   机器学习
date:       2022-7-10
author:     月月鸟
header-img: img/text-classification.png
catalog: true
tags:
    - NLP
---

无监督学习中的**降维**是一种用于在保持数据特征信息的前提下，减少数据维度的方法。降维的目的是简化数据的表示，去除冗余信息，帮助理解和可视化数据，并提高某些机器学习模型的计算效率和性能。下面是对降维的详细解析。

### 1. 为什么需要降维？
- **高维数据的挑战**：
  - **维度诅咒**：随着数据维度增加，数据变得稀疏，距离计算失去意义。
  - **计算资源的消耗**：高维数据带来较大的存储和计算开销。
  - **可视化困难**：高维数据难以在二维或三维空间中直观展示，难以发现其中的模式和结构。

- **降维的目的**：
  - 减少特征数量，减少噪声或冗余特征对模型的影响。
  - 提高数据处理速度和模型的效率。
  - 增强数据的可视化能力，帮助识别模式或趋势。

### 2. 常见的无监督降维方法
无监督学习的降维方法在没有明确标签信息的情况下，通过数据的内部结构来减少维度。主要的无监督降维方法包括**主成分分析（PCA）**、**t-SNE**、**UMAP**等。

#### (1) **主成分分析（PCA, Principal Component Analysis）**
PCA 是一种线性降维方法，旨在通过找到数据的最大方差方向，将数据投影到较少的主成分空间中。

- **原理**：
  通过数据的协方差矩阵计算数据的特征向量和特征值，主成分是最大特征值对应的特征向量，代表数据变化最多的方向。通过保留前几个主成分，可以大幅度减少数据的维度，同时尽可能保持数据的方差信息。

- **优点**：
  - 能够保留尽量多的方差信息。
  - 计算效率高，适合大规模数据集。
  
- **缺点**：
  - 是线性方法，不能很好地处理非线性关系。
  - 可能无法解释保留下来的主成分的实际物理意义。

- **步骤**：
  1. 将数据进行均值归一化。
  2. 计算数据的协方差矩阵。
  3. 计算协方差矩阵的特征值和特征向量。
  4. 选择最大特征值对应的特征向量构成新的低维空间。

#### (2) **t-SNE (t-distributed Stochastic Neighbor Embedding)**
t-SNE 是一种基于概率的降维方法，常用于高维数据的可视化，特别适合处理复杂的非线性数据结构。

- **原理**：
  t-SNE 通过将高维数据中的相似性转换为低维空间中的概率分布，并使得低维空间中的数据点保留高维空间中的局部邻域关系。它在降低维度的同时保持局部结构。

- **优点**：
  - 非常适合数据的可视化，能够展示数据的簇结构。
  - 对非线性数据结构有较强的处理能力。

- **缺点**：
  - 计算复杂度高，不能很好地扩展到大规模数据集。
  - 结果对参数选择比较敏感（如 perplexity 和迭代次数）。

- **步骤**：
  1. 计算高维空间中数据点的相似性（基于高斯分布）。
  2. 计算低维空间中的相似性（基于t分布）。
  3. 通过梯度下降法调整低维点，使得高维和低维的相似性尽可能匹配。

#### (3) **UMAP (Uniform Manifold Approximation and Projection)**
UMAP 是一种基于流形学习的降维方法，能够很好地捕捉非线性数据结构，是 t-SNE 的一种改进，通常用于大规模数据集的降维和可视化。

- **原理**：
  UMAP 假设数据点存在于一个低维流形上，通过近似构建该流形来进行降维。它通过邻居图表征局部结构，并通过优化使低维空间中的结构和高维空间中的结构保持一致。

- **优点**：
  - 速度比 t-SNE 快，适合处理大规模数据集。
  - 保持数据的全局和局部结构，降维后数据的分布更自然。

- **缺点**：
  - 对参数敏感，尤其是邻域大小的选择。
  - 可解释性较差，生成的低维表示难以直接关联到原始特征。

- **步骤**：
  1. 构建 k-近邻图，表示数据的局部结构。
  2. 通过优化近邻图的低维表示，保持原始数据空间的局部拓扑结构。

### 3. 如何选择降维方法？
- 如果你的数据是线性可分的，或者关心数据的全局方差信息，**PCA**是一个不错的选择。
- 如果你的数据存在复杂的非线性结构，并且主要目标是**可视化**，那么**t-SNE**和**UMAP**可能更适合。
- 如果数据规模较大，且对计算时间要求较高，**UMAP**比 t-SNE 更高效。

### 4. 应用场景
- **数据预处理**：通过降维减少特征数量，去除冗余特征，提高模型的训练速度和性能。
- **可视化**：对高维数据进行降维，帮助可视化数据的内在结构，发现数据中的模式或簇。
- **特征提取**：通过降维提取出最具代表性的信息，为后续的机器学习任务提供更有价值的特征。

总之，无监督降维是机器学习和数据分析中的重要工具，能够有效简化复杂数据，同时保持数据的关键结构信息。不同的降维方法适用于不同的数据结构和应用场景，选择合适的方法将显著提升数据处理和建模的效果。

---

# 额外
LDA（**Linear Discriminant Analysis**，线性判别分析）虽然也涉及到降维，但它与主成分分析（PCA）等无监督降维方法不同，LDA是一种**有监督学习**方法。LDA的主要目标是**最大化类间差异**，而非像PCA那样仅关注数据的方差。

### 1. LDA是什么？
LDA是一种有监督的降维方法，主要用于分类任务。它的主要思想是找到一个投影方向，使得同类数据点在该方向上的投影尽可能紧密，而不同类数据点在该方向上的投影尽可能远离。换句话说，LDA是为了**最大化类间方差和类内方差的比值**，从而提高分类模型的效果。

### 2. LDA的工作原理
LDA的核心是通过将高维数据投影到较低维度的线性子空间来进行降维，但它与 PCA 的不同之处在于它利用了类标签信息。LDA通过在投影过程中保持类间的分离度来增强分类效果。

LDA的基本步骤如下：

#### (1) **计算类内和类间的散布矩阵**
- **类内散布矩阵（Within-class scatter matrix）**：衡量同类数据点之间的分布情况。我们希望在降维后，同一类的点能够更靠近。
  
  类内散布矩阵 \( S_W \) 计算公式：

  \[
  S_W = \sum_{i=1}^{C} \sum_{x \in D_i} (x - \mu_i)(x - \mu_i)^T
  \]

  其中，\( C \) 是类的数量，\( D_i \) 是第 \( i \) 类的数据集，\( \mu_i \) 是第 \( i \) 类的均值向量。

- **类间散布矩阵（Between-class scatter matrix）**：衡量不同类的数据点之间的分布情况。我们希望不同类的数据在降维后彼此之间距离更远。

  类间散布矩阵 \( S_B \) 计算公式：

  \[
  S_B = \sum_{i=1}^{C} N_i (\mu_i - \mu)(\mu_i - \mu)^T
  \]

  其中，\( N_i \) 是第 \( i \) 类的样本数量，\( \mu_i \) 是第 \( i \) 类的均值，\( \mu \) 是全局均值。

#### (2) **选择最大化类间散布和最小化类内散布的投影方向**
LDA通过优化一个投影方向，使得类间散布和类内散布的比值最大化。目标函数可以写作：

\[
J(w) = \frac{w^T S_B w}{w^T S_W w}
\]

其中，\( w \) 是投影方向。通过求解这个优化问题，可以找到最佳的投影方向 \( w \)，将高维数据降维。

#### (3) **求解特征值问题**
LDA的优化问题可以通过求解广义特征值问题得到，即：

\[
S_W^{-1} S_B w = \lambda w
\]

其中，\( \lambda \) 是特征值，\( w \) 是对应的特征向量。LDA选择最大的特征值对应的特征向量作为降维方向。

#### (4) **投影数据**
一旦找到了最优投影方向，就可以将原始数据投影到这个低维空间。对于\( k \)个类，LDA最多可以将数据降维到 \( k-1 \) 维。

### 3. LDA与PCA的对比

| **LDA**  | **PCA**  |
|:---------|:---------|
| 有监督学习方法，需要类标签信息。 | 无监督学习方法，不依赖类标签。 |
| 目标是最大化类间分离度和最小化类内差异。 | 目标是最大化数据的整体方差。 |
| 适合分类问题，提升分类性能。 | 适合数据预处理或特征提取，减少特征冗余。 |
| 只保留最多 \( k-1 \) 个维度，\( k \) 是类别数。 | 可以保留任意数量的主成分。 |
| 优化的是类间和类内散布的比值。 | 仅关注整体数据的方差，忽略类别信息。 |

### 4. 适用场景
LDA主要用于有监督的分类任务，尤其是当数据具有标签，并且分类类别较为清晰时，LDA可以帮助增强分类模型的效果。它的常见应用包括：

- **人脸识别**：在人脸识别中，LDA能够通过将人脸特征投影到低维空间，保留类别间的可区分性，增强分类效果。
- **文本分类**：在自然语言处理中的文本分类任务中，LDA可以用于降维和特征提取，减少文本特征的维度，提高分类器的性能。
- **医学诊断**：在医学诊断中，LDA可以帮助分析不同类别的病人数据（如健康与疾病），帮助医生做出决策。

### 5. LDA的局限性
- **线性假设**：LDA假设不同类别的数据点是线性可分的，这对某些复杂的非线性数据可能不适用。
- **样本平衡问题**：如果各类的样本数差异很大，LDA的效果可能会受到影响，可能更偏向于样本数量较大的类别。
- **高维问题**：LDA在处理高维数据时，可能会出现类内散布矩阵不可逆的情况，导致计算问题。

### 6. 总结
LDA是一种有监督的降维技术，特别适合分类问题。通过将数据投影到一个最大化类间差异、最小化类内差异的低维空间，LDA可以有效地提高分类模型的性能。然而，它的线性假设限制了它在处理复杂非线性问题时的表现。在某些情况下，可以结合其他降维方法（如PCA）来增强效果。